{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction ML Aide's purpose is to make model management a joy for you! ML Aide simplifies the documentation process and tracks all relevant information of machine learning model creation. It makes model lifecycle management a joy and enables machine learning operations. ML Aide is deployable anywhere and easy to integrate into the existing enterprise software landscape. Key Benefits ML Aide is designed and developed to be Enterprise-ready: Identity, security, and integrity as a first-class citizen Diverse: Supports every ML library for complete freedom Transparent: Provided open source for maximum transparency Independent: Runs on every cloud platform or on-premises for complete independence Scalable: Scalable from single user to large enterprises and ready for growing demands Effective: Accelerates MLOps for more focus on what really matters: Your business Key Features Track all relevant information of your machine learning models with ML Aide to manage your model lifecycle from training to retirement. Experiment Tracking: Track parameters, metrics, and artifacts in your machine learning experiments that are organized by single runs. Artifact Management: Attach artifacts like code, configs, or models to your experiment runs and reuse them in your next run. Experiment Lineage: Inspect your experiment with a visualized lineage representing the relationship between all runs and artifacts. Model Staging: Put your models under version control and stage them to obtain transparency and reproducibility in your operations. Run Evaluation: Evaluate your runs by viewing or comparing parameters and metrics to identify the best model for your machine learning product. ML Library Integration: An increasing number of integrated machine learning libraries for convenient in-code tracking of parameters, metrics, and models. Access Management: Manage access to your machine learning projects and collaborate with other members of your team. Getting Started Run ML Aide in your local environment. Start Now Tutorial Walk through the model development process with ML Aide. View Tutorial Essentials Learn the essentials of ML Aide. Learn More API Reference Explore the Python SDK or REST API Reference. Explore API Reference","title":"Introduction"},{"location":"#introduction","text":"ML Aide's purpose is to make model management a joy for you! ML Aide simplifies the documentation process and tracks all relevant information of machine learning model creation. It makes model lifecycle management a joy and enables machine learning operations. ML Aide is deployable anywhere and easy to integrate into the existing enterprise software landscape.","title":"Introduction"},{"location":"#key-benefits","text":"ML Aide is designed and developed to be Enterprise-ready: Identity, security, and integrity as a first-class citizen Diverse: Supports every ML library for complete freedom Transparent: Provided open source for maximum transparency Independent: Runs on every cloud platform or on-premises for complete independence Scalable: Scalable from single user to large enterprises and ready for growing demands Effective: Accelerates MLOps for more focus on what really matters: Your business","title":"Key Benefits"},{"location":"#key-features","text":"Track all relevant information of your machine learning models with ML Aide to manage your model lifecycle from training to retirement. Experiment Tracking: Track parameters, metrics, and artifacts in your machine learning experiments that are organized by single runs. Artifact Management: Attach artifacts like code, configs, or models to your experiment runs and reuse them in your next run. Experiment Lineage: Inspect your experiment with a visualized lineage representing the relationship between all runs and artifacts. Model Staging: Put your models under version control and stage them to obtain transparency and reproducibility in your operations. Run Evaluation: Evaluate your runs by viewing or comparing parameters and metrics to identify the best model for your machine learning product. ML Library Integration: An increasing number of integrated machine learning libraries for convenient in-code tracking of parameters, metrics, and models. Access Management: Manage access to your machine learning projects and collaborate with other members of your team.","title":"Key Features"},{"location":"#getting-started","text":"Run ML Aide in your local environment. Start Now","title":"Getting Started"},{"location":"#tutorial","text":"Walk through the model development process with ML Aide. View Tutorial","title":"Tutorial"},{"location":"#essentials","text":"Learn the essentials of ML Aide. Learn More","title":"Essentials"},{"location":"#api-reference","text":"Explore the Python SDK or REST API Reference. Explore API Reference","title":"API Reference"},{"location":"api-reference/python-sdk/","text":"Python SDK MLAideClient This is the main entry point to use this library. Creates a connection to the ML Aide server and provides read and write access to all resources. __init__ ( self , project_key , options = None ) special Creates a new instance of this class. Parameters: Name Type Description Default project_key str The key of the project, that should be accessed. All operations will be made on this project. required options ConnectionOptions Optional options that will be used to establish a connection. None Source code in mlaide/client.py def __init__ ( self , project_key : str , options : ConnectionOptions = None ): \"\"\"Creates a new instance of this class. Arguments: project_key: The key of the project, that should be accessed. All operations will be made on this project. options: Optional options that will be used to establish a connection. \"\"\" if project_key is None : raise ValueError ( \"project key must be not None\" ) self . __project_key = project_key if options is None : self . __options = MLAideClient . __get_default_options () else : self . __options = MLAideClient . __merge_options ( MLAideClient . __get_default_options (), options ) self . __api_client = AuthenticatedClient ( base_url = self . __options . server_url , api_key = self . __options . api_key ) get_artifact ( self , name , version ) Gets an existing artifact. The artifact is specified by its name and version. If no version is specified, the latest available version of the artifact will be used. Parameters: Name Type Description Default name str The name of the artifact. required version Optional[int] The (optional) version of the artifact. If no version is specified, the latest available version required Returns: Type Description ActiveArtifact This object encapsulates an artifact and provides functions to interact with the artifact. Source code in mlaide/client.py def get_artifact ( self , name : str , version : Optional [ int ]) -> ActiveArtifact : \"\"\"Gets an existing artifact. The artifact is specified by its name and version. If no version is specified, the latest available version of the artifact will be used. Arguments: name: The name of the artifact. version: The (optional) version of the artifact. If no version is specified, the latest available version will be loaded. Returns: This object encapsulates an artifact and provides functions to interact with the artifact. \"\"\" return ActiveArtifact ( self . __api_client , self . __project_key , name , version ) load_model ( self , name , version = None , stage = None ) Loads and restores a model. The model is specified by its name and version. If no version is specified, the latest available version of the model will be used. Parameters: Name Type Description Default name str The name of the model. required version Optional[int] The (optional) version of the model. If no version is specified, the latest available version None stage Optional[ModelStage] This argument can only be used when version is None. In this case the latest model can be filtered None Returns: Type Description any The model. E.g. in the case of a scikit-learn model the return value will be a deserialized model that can be used for predictions using .predict(...) . Source code in mlaide/client.py def load_model ( self , name : str , version : Optional [ int ] = None , stage : Optional [ ModelStage ] = None ) -> any : \"\"\"Loads and restores a model. The model is specified by its name and version. If no version is specified, the latest available version of the model will be used. Arguments: name: The name of the model. version: The (optional) version of the model. If no version is specified, the latest available version will be loaded. stage: This argument can only be used when version is None. In this case the latest model can be filtered by its stage. In reverse this means that all model versions will be ignored when they have not the specified stage. Returns: The model. E.g. in the case of a scikit-learn model the return value will be a deserialized model that can be used for predictions using `.predict(...)`. \"\"\" if version is not None and stage is not None : raise ValueError ( \"Only one argument of version and stage can be not None\" ) return ActiveArtifact ( self . __api_client , self . __project_key , name , version , stage ) . load_model () start_new_run ( self , experiment_key = None , run_name = None , used_artifacts = None , auto_create_experiment = True ) Creates and starts a new run, that will be assigned to the specified experiment. The run object can be used to log all necessary information. Parameters: Name Type Description Default experiment_key str The key of the experiment, that the new run should be assigned to. If None a new, random None run_name str The name of the run. The name helps to identify the run for humans. If None a random name will None used_artifacts List[ArtifactRef] An optional list of ArtifactRef that references artifacts, that are used as input for None auto_create_experiment bool Specifies whether the experiment (see experiment_key ) should be created if it True Returns: Type Description ActiveRun This object encapsulates the newly created run and provides functions to log all information that belongs to the run. Source code in mlaide/client.py def start_new_run ( self , experiment_key : str = None , run_name : str = None , used_artifacts : List [ ArtifactRef ] = None , auto_create_experiment : bool = True ) -> ActiveRun : \"\"\"Creates and starts a new run, that will be assigned to the specified experiment. The run object can be used to log all necessary information. Arguments: experiment_key: The key of the experiment, that the new run should be assigned to. If `None` a new, random experiment will be created. run_name: The name of the run. The name helps to identify the run for humans. If `None` a random name will be used. used_artifacts: An optional list of `ArtifactRef` that references artifacts, that are used as input for this run. This information will help to create and visualize the experiment lineage. auto_create_experiment: Specifies whether the experiment (see `experiment_key`) should be created if it does not exist or not. If `auto_create_experiment` is `False` and the experiment does not exist an error will be raised. Returns: This object encapsulates the newly created run and provides functions to log all information \\ that belongs to the run. \"\"\" return ActiveRun ( self . __api_client , self . __project_key , experiment_key , run_name , used_artifacts , auto_create_experiment ) Active Run This class provides access to runs that are stored in ML Aide add_artifact_file ( self , artifact , file , filename = None ) Add a file to an existing artifact. To add multiple file, specify a directory or invoke this function multiple times. Parameters: Name Type Description Default artifact Artifact The artifact to which the file should be added. required file Union[str, _io.BytesIO] The file that should be added. This can be a io.BytesIO object or a string to a file or directory. required filename str The filename. If the file is of type BytesIO the filename must be specified. If the file is a string, the original filename will be the default. None Source code in mlaide/active_run.py def add_artifact_file ( self , artifact : Artifact , file : Union [ str , BytesIO ], filename : str = None ): \"\"\"Add a file to an existing artifact. To add multiple file, specify a directory or invoke this function multiple times. Arguments: artifact: The artifact to which the file should be added. file: The file that should be added. This can be a io.BytesIO object or a string to a file or directory. filename: The filename. If the file is of type BytesIO the filename must be specified. If the file is a string, the original filename will be the default. \"\"\" artifact_api . upload_file ( client = self . __api_client , project_key = self . __project_key , artifact_name = artifact . name , artifact_version = artifact . version , filename = filename if filename is not None else ActiveRun . __extract_filename ( file ), file = ActiveRun . __normalize_file ( file )) create_artifact ( self , name , artifact_type , metadata ) Creates a new artifact. If an artifact with the same name already exists, a new artifact with the next available version number will be registered. Parameters: Name Type Description Default name str The name of the artifact. required artifact_type str The artifact type. required metadata Optional[Dict[str, str]] Some optional metadata that will be attached to the artifact. required Source code in mlaide/active_run.py def create_artifact ( self , name : str , artifact_type : str , metadata : Optional [ Dict [ str , str ]]) -> Artifact : \"\"\"Creates a new artifact. If an artifact with the same name already exists, a new artifact with the next available version number will be registered. Arguments: name: The name of the artifact. artifact_type: The artifact type. metadata: Some optional metadata that will be attached to the artifact. \"\"\" artifact_dto = ArtifactDto ( name = name , type = artifact_type , metadata = metadata , run_key = self . __run . key ) artifact_dto = artifact_api . create_artifact ( client = self . __api_client , project_key = self . __project_key , artifact = artifact_dto ) return dto_to_artifact ( artifact_dto ) log_metric ( self , key , value ) Logs a metric Parameters: Name Type Description Default key str The key of the metric. required value The value of the metric. The value can be any type that is JSON serializable. required Source code in mlaide/active_run.py def log_metric ( self , key : str , value ) -> Run : \"\"\"Logs a metric Arguments: key: The key of the metric. value: The value of the metric. The value can be any type that is JSON serializable. \"\"\" self . __run . metrics [ key ] = value run_api . update_run_metrics ( client = self . __api_client , project_key = self . __project_key , run_key = self . __run . key , metrics = { key : value }) return self . __run log_model ( self , model , model_name , metadata = None ) Creates a new artifact with type 'model'. The artifact will be registered as model. Parameters: Name Type Description Default model The model. The model must be serializable. required model_name str The name of the model. The name will be used as artifact filename. required metadata Optional[Dict[str, str]] Some optional metadata that will be attached to the artifact. None Source code in mlaide/active_run.py def log_model ( self , model , model_name : str , metadata : Optional [ Dict [ str , str ]] = None ): \"\"\"Creates a new artifact with type 'model'. The artifact will be registered as model. Arguments: model: The model. The model must be serializable. model_name: The name of the model. The name will be used as artifact filename. metadata: Some optional metadata that will be attached to the artifact. \"\"\" serialized_model = _model_deser . serialize ( model ) artifact = self . create_artifact ( name = model_name , artifact_type = 'model' , metadata = metadata ) self . add_artifact_file ( artifact = artifact , file = serialized_model , filename = 'model.pkl' ) artifact_api . create_model ( client = self . __api_client , project_key = self . __project_key , artifact_name = artifact . name , artifact_version = artifact . version ) log_parameter ( self , key , value ) Logs a parameter Parameters: Name Type Description Default key str The key of the parameter. required value The value of the parameter. The value must be a scalar value (e.g. string, int, float, ...). required Source code in mlaide/active_run.py def log_parameter ( self , key : str , value ) -> Run : \"\"\"Logs a parameter Arguments: key: The key of the parameter. value: The value of the parameter. The value must be a scalar value (e.g. string, int, float, ...). \"\"\" self . __run . parameters [ key ] = value run_api . update_run_parameters ( client = self . __api_client , project_key = self . __project_key , run_key = self . __run . key , parameters = { key : value }) return self . __run set_completed_status ( self ) Sets the status of the current run as completed. Source code in mlaide/active_run.py def set_completed_status ( self ) -> Run : \"\"\"Sets the status of the current run as completed.\"\"\" return self . _set_status ( RunStatus . COMPLETED ) set_failed_status ( self ) Sets the status of the current run as failed. Source code in mlaide/active_run.py def set_failed_status ( self ) -> Run : \"\"\"Sets the status of the current run as failed.\"\"\" return self . _set_status ( RunStatus . FAILED ) Active Artifact This class provides access to artifacts that are stored in ML Aide download ( self , target_directory ) Downloads all files of this artifact and stores them into the specified directory. Parameters: Name Type Description Default target_directory str The path to the directory where all files should be stored. required Source code in mlaide/active_artifact.py def download ( self , target_directory : str ): \"\"\"Downloads all files of this artifact and stores them into the specified directory. Arguments: target_directory: The path to the directory where all files should be stored. \"\"\" # download artifact_bytes , artifact_filename = self . __download_zip () # unzip and write to disk with ZipFile ( artifact_bytes ) as z : z . extractall ( target_directory ) load ( self , filename ) Load a specific file of this artifact into memory Parameters: Name Type Description Default filename str The name of the file that should be loaded required Source code in mlaide/active_artifact.py def load ( self , filename : str ) -> BytesIO : \"\"\"Load a specific file of this artifact into memory Arguments: filename: The name of the file that should be loaded \"\"\" # TODO: Do not download whole zip; instead download just the single file zip_bytes , zip_filename = self . __download_zip () with ZipFile ( zip_bytes ) as z : zip_info = z . infolist () desired_file = next ( info for info in zip_info if info . filename == filename ) with z . open ( desired_file , 'r' ) as zip_file : return BytesIO ( zip_file . read ())","title":"Python SDK"},{"location":"api-reference/python-sdk/#python-sdk","text":"","title":"Python SDK"},{"location":"api-reference/python-sdk/#mlaideclient","text":"This is the main entry point to use this library. Creates a connection to the ML Aide server and provides read and write access to all resources.","title":"MLAideClient"},{"location":"api-reference/python-sdk/#mlaide.client.MLAideClient.__init__","text":"Creates a new instance of this class. Parameters: Name Type Description Default project_key str The key of the project, that should be accessed. All operations will be made on this project. required options ConnectionOptions Optional options that will be used to establish a connection. None Source code in mlaide/client.py def __init__ ( self , project_key : str , options : ConnectionOptions = None ): \"\"\"Creates a new instance of this class. Arguments: project_key: The key of the project, that should be accessed. All operations will be made on this project. options: Optional options that will be used to establish a connection. \"\"\" if project_key is None : raise ValueError ( \"project key must be not None\" ) self . __project_key = project_key if options is None : self . __options = MLAideClient . __get_default_options () else : self . __options = MLAideClient . __merge_options ( MLAideClient . __get_default_options (), options ) self . __api_client = AuthenticatedClient ( base_url = self . __options . server_url , api_key = self . __options . api_key )","title":"__init__()"},{"location":"api-reference/python-sdk/#mlaide.client.MLAideClient.get_artifact","text":"Gets an existing artifact. The artifact is specified by its name and version. If no version is specified, the latest available version of the artifact will be used. Parameters: Name Type Description Default name str The name of the artifact. required version Optional[int] The (optional) version of the artifact. If no version is specified, the latest available version required Returns: Type Description ActiveArtifact This object encapsulates an artifact and provides functions to interact with the artifact. Source code in mlaide/client.py def get_artifact ( self , name : str , version : Optional [ int ]) -> ActiveArtifact : \"\"\"Gets an existing artifact. The artifact is specified by its name and version. If no version is specified, the latest available version of the artifact will be used. Arguments: name: The name of the artifact. version: The (optional) version of the artifact. If no version is specified, the latest available version will be loaded. Returns: This object encapsulates an artifact and provides functions to interact with the artifact. \"\"\" return ActiveArtifact ( self . __api_client , self . __project_key , name , version )","title":"get_artifact()"},{"location":"api-reference/python-sdk/#mlaide.client.MLAideClient.load_model","text":"Loads and restores a model. The model is specified by its name and version. If no version is specified, the latest available version of the model will be used. Parameters: Name Type Description Default name str The name of the model. required version Optional[int] The (optional) version of the model. If no version is specified, the latest available version None stage Optional[ModelStage] This argument can only be used when version is None. In this case the latest model can be filtered None Returns: Type Description any The model. E.g. in the case of a scikit-learn model the return value will be a deserialized model that can be used for predictions using .predict(...) . Source code in mlaide/client.py def load_model ( self , name : str , version : Optional [ int ] = None , stage : Optional [ ModelStage ] = None ) -> any : \"\"\"Loads and restores a model. The model is specified by its name and version. If no version is specified, the latest available version of the model will be used. Arguments: name: The name of the model. version: The (optional) version of the model. If no version is specified, the latest available version will be loaded. stage: This argument can only be used when version is None. In this case the latest model can be filtered by its stage. In reverse this means that all model versions will be ignored when they have not the specified stage. Returns: The model. E.g. in the case of a scikit-learn model the return value will be a deserialized model that can be used for predictions using `.predict(...)`. \"\"\" if version is not None and stage is not None : raise ValueError ( \"Only one argument of version and stage can be not None\" ) return ActiveArtifact ( self . __api_client , self . __project_key , name , version , stage ) . load_model ()","title":"load_model()"},{"location":"api-reference/python-sdk/#mlaide.client.MLAideClient.start_new_run","text":"Creates and starts a new run, that will be assigned to the specified experiment. The run object can be used to log all necessary information. Parameters: Name Type Description Default experiment_key str The key of the experiment, that the new run should be assigned to. If None a new, random None run_name str The name of the run. The name helps to identify the run for humans. If None a random name will None used_artifacts List[ArtifactRef] An optional list of ArtifactRef that references artifacts, that are used as input for None auto_create_experiment bool Specifies whether the experiment (see experiment_key ) should be created if it True Returns: Type Description ActiveRun This object encapsulates the newly created run and provides functions to log all information that belongs to the run. Source code in mlaide/client.py def start_new_run ( self , experiment_key : str = None , run_name : str = None , used_artifacts : List [ ArtifactRef ] = None , auto_create_experiment : bool = True ) -> ActiveRun : \"\"\"Creates and starts a new run, that will be assigned to the specified experiment. The run object can be used to log all necessary information. Arguments: experiment_key: The key of the experiment, that the new run should be assigned to. If `None` a new, random experiment will be created. run_name: The name of the run. The name helps to identify the run for humans. If `None` a random name will be used. used_artifacts: An optional list of `ArtifactRef` that references artifacts, that are used as input for this run. This information will help to create and visualize the experiment lineage. auto_create_experiment: Specifies whether the experiment (see `experiment_key`) should be created if it does not exist or not. If `auto_create_experiment` is `False` and the experiment does not exist an error will be raised. Returns: This object encapsulates the newly created run and provides functions to log all information \\ that belongs to the run. \"\"\" return ActiveRun ( self . __api_client , self . __project_key , experiment_key , run_name , used_artifacts , auto_create_experiment )","title":"start_new_run()"},{"location":"api-reference/python-sdk/#active-run","text":"This class provides access to runs that are stored in ML Aide","title":"Active Run"},{"location":"api-reference/python-sdk/#mlaide.active_run.ActiveRun.add_artifact_file","text":"Add a file to an existing artifact. To add multiple file, specify a directory or invoke this function multiple times. Parameters: Name Type Description Default artifact Artifact The artifact to which the file should be added. required file Union[str, _io.BytesIO] The file that should be added. This can be a io.BytesIO object or a string to a file or directory. required filename str The filename. If the file is of type BytesIO the filename must be specified. If the file is a string, the original filename will be the default. None Source code in mlaide/active_run.py def add_artifact_file ( self , artifact : Artifact , file : Union [ str , BytesIO ], filename : str = None ): \"\"\"Add a file to an existing artifact. To add multiple file, specify a directory or invoke this function multiple times. Arguments: artifact: The artifact to which the file should be added. file: The file that should be added. This can be a io.BytesIO object or a string to a file or directory. filename: The filename. If the file is of type BytesIO the filename must be specified. If the file is a string, the original filename will be the default. \"\"\" artifact_api . upload_file ( client = self . __api_client , project_key = self . __project_key , artifact_name = artifact . name , artifact_version = artifact . version , filename = filename if filename is not None else ActiveRun . __extract_filename ( file ), file = ActiveRun . __normalize_file ( file ))","title":"add_artifact_file()"},{"location":"api-reference/python-sdk/#mlaide.active_run.ActiveRun.create_artifact","text":"Creates a new artifact. If an artifact with the same name already exists, a new artifact with the next available version number will be registered. Parameters: Name Type Description Default name str The name of the artifact. required artifact_type str The artifact type. required metadata Optional[Dict[str, str]] Some optional metadata that will be attached to the artifact. required Source code in mlaide/active_run.py def create_artifact ( self , name : str , artifact_type : str , metadata : Optional [ Dict [ str , str ]]) -> Artifact : \"\"\"Creates a new artifact. If an artifact with the same name already exists, a new artifact with the next available version number will be registered. Arguments: name: The name of the artifact. artifact_type: The artifact type. metadata: Some optional metadata that will be attached to the artifact. \"\"\" artifact_dto = ArtifactDto ( name = name , type = artifact_type , metadata = metadata , run_key = self . __run . key ) artifact_dto = artifact_api . create_artifact ( client = self . __api_client , project_key = self . __project_key , artifact = artifact_dto ) return dto_to_artifact ( artifact_dto )","title":"create_artifact()"},{"location":"api-reference/python-sdk/#mlaide.active_run.ActiveRun.log_metric","text":"Logs a metric Parameters: Name Type Description Default key str The key of the metric. required value The value of the metric. The value can be any type that is JSON serializable. required Source code in mlaide/active_run.py def log_metric ( self , key : str , value ) -> Run : \"\"\"Logs a metric Arguments: key: The key of the metric. value: The value of the metric. The value can be any type that is JSON serializable. \"\"\" self . __run . metrics [ key ] = value run_api . update_run_metrics ( client = self . __api_client , project_key = self . __project_key , run_key = self . __run . key , metrics = { key : value }) return self . __run","title":"log_metric()"},{"location":"api-reference/python-sdk/#mlaide.active_run.ActiveRun.log_model","text":"Creates a new artifact with type 'model'. The artifact will be registered as model. Parameters: Name Type Description Default model The model. The model must be serializable. required model_name str The name of the model. The name will be used as artifact filename. required metadata Optional[Dict[str, str]] Some optional metadata that will be attached to the artifact. None Source code in mlaide/active_run.py def log_model ( self , model , model_name : str , metadata : Optional [ Dict [ str , str ]] = None ): \"\"\"Creates a new artifact with type 'model'. The artifact will be registered as model. Arguments: model: The model. The model must be serializable. model_name: The name of the model. The name will be used as artifact filename. metadata: Some optional metadata that will be attached to the artifact. \"\"\" serialized_model = _model_deser . serialize ( model ) artifact = self . create_artifact ( name = model_name , artifact_type = 'model' , metadata = metadata ) self . add_artifact_file ( artifact = artifact , file = serialized_model , filename = 'model.pkl' ) artifact_api . create_model ( client = self . __api_client , project_key = self . __project_key , artifact_name = artifact . name , artifact_version = artifact . version )","title":"log_model()"},{"location":"api-reference/python-sdk/#mlaide.active_run.ActiveRun.log_parameter","text":"Logs a parameter Parameters: Name Type Description Default key str The key of the parameter. required value The value of the parameter. The value must be a scalar value (e.g. string, int, float, ...). required Source code in mlaide/active_run.py def log_parameter ( self , key : str , value ) -> Run : \"\"\"Logs a parameter Arguments: key: The key of the parameter. value: The value of the parameter. The value must be a scalar value (e.g. string, int, float, ...). \"\"\" self . __run . parameters [ key ] = value run_api . update_run_parameters ( client = self . __api_client , project_key = self . __project_key , run_key = self . __run . key , parameters = { key : value }) return self . __run","title":"log_parameter()"},{"location":"api-reference/python-sdk/#mlaide.active_run.ActiveRun.set_completed_status","text":"Sets the status of the current run as completed. Source code in mlaide/active_run.py def set_completed_status ( self ) -> Run : \"\"\"Sets the status of the current run as completed.\"\"\" return self . _set_status ( RunStatus . COMPLETED )","title":"set_completed_status()"},{"location":"api-reference/python-sdk/#mlaide.active_run.ActiveRun.set_failed_status","text":"Sets the status of the current run as failed. Source code in mlaide/active_run.py def set_failed_status ( self ) -> Run : \"\"\"Sets the status of the current run as failed.\"\"\" return self . _set_status ( RunStatus . FAILED )","title":"set_failed_status()"},{"location":"api-reference/python-sdk/#active-artifact","text":"This class provides access to artifacts that are stored in ML Aide","title":"Active Artifact"},{"location":"api-reference/python-sdk/#mlaide.active_artifact.ActiveArtifact.download","text":"Downloads all files of this artifact and stores them into the specified directory. Parameters: Name Type Description Default target_directory str The path to the directory where all files should be stored. required Source code in mlaide/active_artifact.py def download ( self , target_directory : str ): \"\"\"Downloads all files of this artifact and stores them into the specified directory. Arguments: target_directory: The path to the directory where all files should be stored. \"\"\" # download artifact_bytes , artifact_filename = self . __download_zip () # unzip and write to disk with ZipFile ( artifact_bytes ) as z : z . extractall ( target_directory )","title":"download()"},{"location":"api-reference/python-sdk/#mlaide.active_artifact.ActiveArtifact.load","text":"Load a specific file of this artifact into memory Parameters: Name Type Description Default filename str The name of the file that should be loaded required Source code in mlaide/active_artifact.py def load ( self , filename : str ) -> BytesIO : \"\"\"Load a specific file of this artifact into memory Arguments: filename: The name of the file that should be loaded \"\"\" # TODO: Do not download whole zip; instead download just the single file zip_bytes , zip_filename = self . __download_zip () with ZipFile ( zip_bytes ) as z : zip_info = z . infolist () desired_file = next ( info for info in zip_info if info . filename == filename ) with z . open ( desired_file , 'r' ) as zip_file : return BytesIO ( zip_file . read ())","title":"load()"},{"location":"api-reference/rest-api/","text":"REST API const ui = SwaggerUIBundle({ url: 'swagger.yml', dom_id: '#swagger-ui', })","title":"REST API"},{"location":"api-reference/rest-api/#rest-api","text":"const ui = SwaggerUIBundle({ url: 'swagger.yml', dom_id: '#swagger-ui', })","title":"REST API"},{"location":"architecture/","text":"Overview Work in Progress This chapter describes the architectural core concepts of ML Aide.","title":"Overview"},{"location":"architecture/#overview","text":"","title":"Overview"},{"location":"architecture/#work-in-progress","text":"This chapter describes the architectural core concepts of ML Aide.","title":"Work in Progress"},{"location":"essentials/","text":"Overview This chapter describes the essentials of projects, experiments, runs, and their connection with users, artifacts, and models. Projects Projects are at the top hierarchy level and group several experiments that belong to it. Projects represent a business problem that shall be solved with a machine learning model that is created through the underlying experiments. Example Project: House Price Prediction Please see the projects page for further details. Experiments Experiments belong to exactly one project and group several runs that belong to it. Experiments represent an attempt to solve the business problem with the underlying runs that generate several artifacts as well as models. Example Project: House Price Prediction Experiment 1: Linear Regression Experiment 2: Random Forest Regression Experiment 3: Random Forest Regression with HP tuning Please see the experiments page for further details. Runs Runs belong to exactly one project and none, one or many experiments. However, it is good practice to assign a run to at least one experiment. They may have input from previous runs and may generate outputs \u2013 named artifacts \u2013 such as models. Runs also contain parameters and metrics that are key-value pairs to make them comparable. Example Project: House Price Prediction Experiment 1: Linear Regression Run 1: Data Preparation Run 2: Test Train Split Run 3: Linear Regression Training Experiment 2: Random Forest Regression Run 1: Data Preparation Run 2: Test Train Split Run 4: Random Forest Regression Training Please see the runs page for further details. Artifacts Artifacts are generated by runs and may consist of one or a group of files. They have a type that can be freely assigned as well as the artifact name. Example Experiment 1: Linear Regression Run 1: Data Preparation Artifact 1: housing.csv Run 2: Test Train Split Artifact 1: Pipeline Run 3: Linear Regression Training Artifact 2: Model Please see the artifacts page for further details. Models Models are a special type of artifact named model and have a special status in ML Aide since they are the heart of the application. Example Experiment 1: Linear Regression Run 1: Linear Regression Training 1 Artifact 1: Model 1 Run 2: Linear Regression Training 2 Artifact 1: Model 2 Please see the models page for further details. Project Settings Project members can be managed via project settings. The following roles are available: Owner Contributor Viewer Please see the project settings page for further details. Users ML Aide provides internal user management that allows to Update personal information Manage API Keys Please see the users page for further details.","title":"Overview"},{"location":"essentials/#overview","text":"This chapter describes the essentials of projects, experiments, runs, and their connection with users, artifacts, and models.","title":"Overview"},{"location":"essentials/#projects","text":"Projects are at the top hierarchy level and group several experiments that belong to it. Projects represent a business problem that shall be solved with a machine learning model that is created through the underlying experiments. Example Project: House Price Prediction Please see the projects page for further details.","title":"Projects"},{"location":"essentials/#experiments","text":"Experiments belong to exactly one project and group several runs that belong to it. Experiments represent an attempt to solve the business problem with the underlying runs that generate several artifacts as well as models. Example Project: House Price Prediction Experiment 1: Linear Regression Experiment 2: Random Forest Regression Experiment 3: Random Forest Regression with HP tuning Please see the experiments page for further details.","title":"Experiments"},{"location":"essentials/#runs","text":"Runs belong to exactly one project and none, one or many experiments. However, it is good practice to assign a run to at least one experiment. They may have input from previous runs and may generate outputs \u2013 named artifacts \u2013 such as models. Runs also contain parameters and metrics that are key-value pairs to make them comparable. Example Project: House Price Prediction Experiment 1: Linear Regression Run 1: Data Preparation Run 2: Test Train Split Run 3: Linear Regression Training Experiment 2: Random Forest Regression Run 1: Data Preparation Run 2: Test Train Split Run 4: Random Forest Regression Training Please see the runs page for further details.","title":"Runs"},{"location":"essentials/#artifacts","text":"Artifacts are generated by runs and may consist of one or a group of files. They have a type that can be freely assigned as well as the artifact name. Example Experiment 1: Linear Regression Run 1: Data Preparation Artifact 1: housing.csv Run 2: Test Train Split Artifact 1: Pipeline Run 3: Linear Regression Training Artifact 2: Model Please see the artifacts page for further details.","title":"Artifacts"},{"location":"essentials/#models","text":"Models are a special type of artifact named model and have a special status in ML Aide since they are the heart of the application. Example Experiment 1: Linear Regression Run 1: Linear Regression Training 1 Artifact 1: Model 1 Run 2: Linear Regression Training 2 Artifact 1: Model 2 Please see the models page for further details.","title":"Models"},{"location":"essentials/#project-settings","text":"Project members can be managed via project settings. The following roles are available: Owner Contributor Viewer Please see the project settings page for further details.","title":"Project Settings"},{"location":"essentials/#users","text":"ML Aide provides internal user management that allows to Update personal information Manage API Keys Please see the users page for further details.","title":"Users"},{"location":"essentials/artifacts/","text":"Artifacts Overview Artifacts are generated by runs and may consist of one or a group of files. They have a type that can be freely assigned as well as the artifact name. Example Experiment 1: Linear Regression Run 1: Data Preparation Artifact 1: housing.csv Run 2: Test Train Split Artifact 1: Pipeline Run 3: Linear Regression Training Artifact 2: Model Features Show Artifacts Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Artifacts button in the side navigation Create Artifact Instructions Code artifact = run . create_artifact ( name = \"my-dataset\" , artifact_type = \"dataset\" , metadata = {}) run . add_artifact_file ( artifact , 'dataset.csv' )","title":"Artifacts"},{"location":"essentials/artifacts/#artifacts","text":"","title":"Artifacts"},{"location":"essentials/artifacts/#overview","text":"Artifacts are generated by runs and may consist of one or a group of files. They have a type that can be freely assigned as well as the artifact name. Example Experiment 1: Linear Regression Run 1: Data Preparation Artifact 1: housing.csv Run 2: Test Train Split Artifact 1: Pipeline Run 3: Linear Regression Training Artifact 2: Model","title":"Overview"},{"location":"essentials/artifacts/#features","text":"","title":"Features"},{"location":"essentials/artifacts/#show-artifacts","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Artifacts button in the side navigation","title":"Show Artifacts"},{"location":"essentials/artifacts/#create-artifact","text":"Instructions Code artifact = run . create_artifact ( name = \"my-dataset\" , artifact_type = \"dataset\" , metadata = {}) run . add_artifact_file ( artifact , 'dataset.csv' )","title":"Create Artifact"},{"location":"essentials/experiments/","text":"Experiments Overview Experiments belong to exactly one project and group several runs that belong to it. Experiments represent an attempt to solve the business problem with the underlying runs that generate several artifacts as well as models. Example Project: House Price Prediction Experiment 1: Linear Regression Experiment 2: Random Forest Regression Experiment 3: Random Forest Regression with HP tuning Features Show Experiments Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Experiments button in the side navigation Create Experiment Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Experiments button in the side navigation Click the Add Experiment button Provide Experiment name Experiment key Experiment tags - optional Experiment status Confirm by clicking the Create button Edit Experiment Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Experiments button in the side navigation Click the Edit Experiment button ( ) for the relevant experiment Change Experiment name Experiment tags Experiment status Confirm by clicking the Update button Show Details Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Experiments button in the side navigation Click the name of the relevant experiment","title":"Experiments"},{"location":"essentials/experiments/#experiments","text":"","title":"Experiments"},{"location":"essentials/experiments/#overview","text":"Experiments belong to exactly one project and group several runs that belong to it. Experiments represent an attempt to solve the business problem with the underlying runs that generate several artifacts as well as models. Example Project: House Price Prediction Experiment 1: Linear Regression Experiment 2: Random Forest Regression Experiment 3: Random Forest Regression with HP tuning","title":"Overview"},{"location":"essentials/experiments/#features","text":"","title":"Features"},{"location":"essentials/experiments/#show-experiments","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Experiments button in the side navigation","title":"Show Experiments"},{"location":"essentials/experiments/#create-experiment","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Experiments button in the side navigation Click the Add Experiment button Provide Experiment name Experiment key Experiment tags - optional Experiment status Confirm by clicking the Create button","title":"Create Experiment"},{"location":"essentials/experiments/#edit-experiment","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Experiments button in the side navigation Click the Edit Experiment button ( ) for the relevant experiment Change Experiment name Experiment tags Experiment status Confirm by clicking the Update button","title":"Edit Experiment"},{"location":"essentials/experiments/#show-details","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Experiments button in the side navigation Click the name of the relevant experiment","title":"Show Details"},{"location":"essentials/models/","text":"Models Overview Models are a special type of artifact named model and have a special status in ML Aide since they are the heart of the application. Example Experiment 1: Linear Regression Run 1: Linear Regression Training 1 Artifact 1: Model 1 Run 2: Linear Regression Training 2 Artifact 1: Model 2 Show Models Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Models button in the side navigation Create Model Instructions Code lin_reg = LinearRegression () lin_reg . fit ( X_train , y_train ) run . log_model ( lin_reg , model_name = \"linear regression model\" ) Edit Model Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Models button in the side navigation Click the Edit Model button ( ) for the relevant model Change Model stage Note - optional Confirm by clicking the Update button Show Stage Log Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Models button in the side navigation Click the Stage log button ( ) for the relevant model Close by clicking the Close button","title":"Models"},{"location":"essentials/models/#models","text":"","title":"Models"},{"location":"essentials/models/#overview","text":"Models are a special type of artifact named model and have a special status in ML Aide since they are the heart of the application. Example Experiment 1: Linear Regression Run 1: Linear Regression Training 1 Artifact 1: Model 1 Run 2: Linear Regression Training 2 Artifact 1: Model 2","title":"Overview"},{"location":"essentials/models/#show-models","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Models button in the side navigation","title":"Show Models"},{"location":"essentials/models/#create-model","text":"Instructions Code lin_reg = LinearRegression () lin_reg . fit ( X_train , y_train ) run . log_model ( lin_reg , model_name = \"linear regression model\" )","title":"Create Model"},{"location":"essentials/models/#edit-model","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Models button in the side navigation Click the Edit Model button ( ) for the relevant model Change Model stage Note - optional Confirm by clicking the Update button","title":"Edit Model"},{"location":"essentials/models/#show-stage-log","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Models button in the side navigation Click the Stage log button ( ) for the relevant model Close by clicking the Close button","title":"Show Stage Log"},{"location":"essentials/project-settings/","text":"Project Settings Project members can be managed via project settings. The following roles are available: Owner Contributor Viewer Roles & Rights Overview TODO: Check create roles & rights overview as table Features Show Settings Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Settings button in the side navigation Add Project Members Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Settings button in the side navigation Click the Add Project Member button Provide Email Role Confirm by clicking the Create button User has to log in at least once before he/she can be added as a project member Edit Project Members Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Settings button in the side navigation Click the Edit Project Member button ( ) for the relevant project member Change Role Confirm by clicking the Update button","title":"Project Settings"},{"location":"essentials/project-settings/#project-settings","text":"Project members can be managed via project settings. The following roles are available: Owner Contributor Viewer","title":"Project Settings"},{"location":"essentials/project-settings/#roles-rights-overview","text":"TODO: Check create roles & rights overview as table","title":"Roles &amp; Rights Overview"},{"location":"essentials/project-settings/#features","text":"","title":"Features"},{"location":"essentials/project-settings/#show-settings","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Settings button in the side navigation","title":"Show Settings"},{"location":"essentials/project-settings/#add-project-members","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Settings button in the side navigation Click the Add Project Member button Provide Email Role Confirm by clicking the Create button User has to log in at least once before he/she can be added as a project member","title":"Add Project Members"},{"location":"essentials/project-settings/#edit-project-members","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Settings button in the side navigation Click the Edit Project Member button ( ) for the relevant project member Change Role Confirm by clicking the Update button","title":"Edit Project Members"},{"location":"essentials/projects/","text":"Projects Overview Projects are at the top hierarchy level and group several experiments that belong to it. Projects represent a business problem that shall be solved with a machine learning model that is created through the underlying experiments. Example Project: House Price Prediction Features Show Projects Instructions GUI Click the ML Aide button in the upper left corner Or Click the Projects dropdown in the main navigation and click Show all Create Project Instructions GUI Click the ML Aide button in the upper left corner Click the Add Project button Provide Project name Project key Confirm by clicking the Create button","title":"Projects"},{"location":"essentials/projects/#projects","text":"","title":"Projects"},{"location":"essentials/projects/#overview","text":"Projects are at the top hierarchy level and group several experiments that belong to it. Projects represent a business problem that shall be solved with a machine learning model that is created through the underlying experiments. Example Project: House Price Prediction","title":"Overview"},{"location":"essentials/projects/#features","text":"","title":"Features"},{"location":"essentials/projects/#show-projects","text":"Instructions GUI Click the ML Aide button in the upper left corner Or Click the Projects dropdown in the main navigation and click Show all","title":"Show Projects"},{"location":"essentials/projects/#create-project","text":"Instructions GUI Click the ML Aide button in the upper left corner Click the Add Project button Provide Project name Project key Confirm by clicking the Create button","title":"Create Project"},{"location":"essentials/runs/","text":"Runs Overview Runs belong to exactly one project and none, one or many experiments. However, it is good practice to assign a run to at least one experiment. They may have input from previous runs and may generate outputs \u2013 named artifacts \u2013 such as models. Runs also contain parameters and metrics that are key-value pairs to make them comparable. Example Project: House Price Prediction Experiment 1: Linear Regression Run 1: Data Preparation Run 2: Test Train Split Run 3: Linear Regression Training Experiment 2: Random Forest Regression Run 1: Data Preparation Run 2: Test Train Split Run 4: Random Forest Regression Training Features Show Runs Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Runs button in the side navigation Create Run Instructions Code run = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'linear regression' ) Toggle Paramenters Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Runs button in the side navigation Click the Show Parameters button at the top of the runs table to show parameters for runs Click the Hide Parameters button at the top of the runs table to hide parameters for runs Compare Runs Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Runs button in the side navigation Select the relevant runs by clicking the checkbox Click the Compare button at the top of the runs table Export Runs Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Runs button in the side navigation Select the relevant runs by clicking the checkbox Click the Export button at the top of the runs table","title":"Runs"},{"location":"essentials/runs/#runs","text":"","title":"Runs"},{"location":"essentials/runs/#overview","text":"Runs belong to exactly one project and none, one or many experiments. However, it is good practice to assign a run to at least one experiment. They may have input from previous runs and may generate outputs \u2013 named artifacts \u2013 such as models. Runs also contain parameters and metrics that are key-value pairs to make them comparable. Example Project: House Price Prediction Experiment 1: Linear Regression Run 1: Data Preparation Run 2: Test Train Split Run 3: Linear Regression Training Experiment 2: Random Forest Regression Run 1: Data Preparation Run 2: Test Train Split Run 4: Random Forest Regression Training","title":"Overview"},{"location":"essentials/runs/#features","text":"","title":"Features"},{"location":"essentials/runs/#show-runs","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Runs button in the side navigation","title":"Show Runs"},{"location":"essentials/runs/#create-run","text":"Instructions Code run = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'linear regression' )","title":"Create Run"},{"location":"essentials/runs/#toggle-paramenters","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Runs button in the side navigation Click the Show Parameters button at the top of the runs table to show parameters for runs Click the Hide Parameters button at the top of the runs table to hide parameters for runs","title":"Toggle Paramenters"},{"location":"essentials/runs/#compare-runs","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Runs button in the side navigation Select the relevant runs by clicking the checkbox Click the Compare button at the top of the runs table","title":"Compare Runs"},{"location":"essentials/runs/#export-runs","text":"Instructions GUI Select the relevant project by clicking it in the home view or via the Projects dropdown in the main navigation Click the Runs button in the side navigation Select the relevant runs by clicking the checkbox Click the Export button at the top of the runs table","title":"Export Runs"},{"location":"essentials/users/","text":"Users Overview ML Aide provides internal user management that allows to Update personal information Manage API Keys Features Show User Settings Instructions GUI Click the User name button in the upper right corner to open the user drop-down menu Click the Settings button Update User Profile Instructions GUI Click the User name button in the upper right corner to open the user drop-down menu Click the Settings button Click the Profile button in the side navigation Change First name - optional Last name - optional Nickname Confirm by clicking the Save button Add API Key Instructions GUI Click the User name button in the upper right corner to open the user drop-down menu Click the Settings button Click the API Keys button in the side navigation Click the Add API Key button Provide Description - optional Expires at - optional Confirm by clicking the Create button Use API Key Instructions Code options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) Delete API Key Instructions GUI Click the User name button in the upper right corner to open the user drop-down menu Click the Settings button Click the API Keys button in the side navigation Click the Delete API Key button ( ) for the relevant API key","title":"Users"},{"location":"essentials/users/#users","text":"","title":"Users"},{"location":"essentials/users/#overview","text":"ML Aide provides internal user management that allows to Update personal information Manage API Keys","title":"Overview"},{"location":"essentials/users/#features","text":"","title":"Features"},{"location":"essentials/users/#show-user-settings","text":"Instructions GUI Click the User name button in the upper right corner to open the user drop-down menu Click the Settings button","title":"Show User Settings"},{"location":"essentials/users/#update-user-profile","text":"Instructions GUI Click the User name button in the upper right corner to open the user drop-down menu Click the Settings button Click the Profile button in the side navigation Change First name - optional Last name - optional Nickname Confirm by clicking the Save button","title":"Update User Profile"},{"location":"essentials/users/#add-api-key","text":"Instructions GUI Click the User name button in the upper right corner to open the user drop-down menu Click the Settings button Click the API Keys button in the side navigation Click the Add API Key button Provide Description - optional Expires at - optional Confirm by clicking the Create button","title":"Add API Key"},{"location":"essentials/users/#use-api-key","text":"Instructions Code options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options )","title":"Use API Key"},{"location":"essentials/users/#delete-api-key","text":"Instructions GUI Click the User name button in the upper right corner to open the user drop-down menu Click the Settings button Click the API Keys button in the side navigation Click the Delete API Key button ( ) for the relevant API key","title":"Delete API Key"},{"location":"start/environment-setup/","text":"Environment Setup Work in Progress","title":"Environment Setup"},{"location":"start/environment-setup/#environment-setup","text":"","title":"Environment Setup"},{"location":"start/environment-setup/#work-in-progress","text":"","title":"Work in Progress"},{"location":"start/installation-on-localhost/","text":"Installation on localhost This guide shows how to install ML Aide on your local environment. This guide is only for demo purposes and should not be used for a production setup. ML Aide is shipped with Docker. Therefore you need Docker installed. On Linux, you also have to install Docker Compose . Clone Repo Clone the ML Aide git repository git clone https://github.com/MLAide/MLAide.git For the installation, you don't have to compile or build anything. We only need the docker-compose.yaml from the demo/ directory. Prepare Add the following entry to your hosts file, to make sure keycloack can verify the user tokens. The file is located at /etc/hosts (Unix) or C:\\Windows\\System32\\drivers\\etc\\hosts (Windows). 127.0.0.1 keycloak.mlaide On Unix systems, you can use the following command to add the entry to your hosts file. echo '127.0.0.1 keycloak.mlaide' | sudo tee -a /etc/hosts Start After that start ML Aide from your demo/ folder that is located in the cloned repository using: cd demo docker-compose up Verify Now you should have several containers running: MLAide web UI MLAide webserver MongoDB which is used by the webserver to store all structured metadata min.io that is used by the webserver to store all artifacts and models keycloak to provide an identity provider, authentication, and authorization Use You can access the web UI on localhost:8880 with your browser. This demo provides three pre-defined users: adam (password = adam1, email = adam@demo.mlaide.com) bob (password = bob1, email = bob@demo.mlaide.com) eve (password = eve1, email = eve@demo.mlaide.com) Using the Python client To track your machine learning experiments you can use the Python client. Start the Tutorial for your first steps.","title":"Installation on localhost"},{"location":"start/installation-on-localhost/#installation-on-localhost","text":"This guide shows how to install ML Aide on your local environment. This guide is only for demo purposes and should not be used for a production setup. ML Aide is shipped with Docker. Therefore you need Docker installed. On Linux, you also have to install Docker Compose .","title":"Installation on localhost"},{"location":"start/installation-on-localhost/#clone-repo","text":"Clone the ML Aide git repository git clone https://github.com/MLAide/MLAide.git For the installation, you don't have to compile or build anything. We only need the docker-compose.yaml from the demo/ directory.","title":"Clone Repo"},{"location":"start/installation-on-localhost/#prepare","text":"Add the following entry to your hosts file, to make sure keycloack can verify the user tokens. The file is located at /etc/hosts (Unix) or C:\\Windows\\System32\\drivers\\etc\\hosts (Windows). 127.0.0.1 keycloak.mlaide On Unix systems, you can use the following command to add the entry to your hosts file. echo '127.0.0.1 keycloak.mlaide' | sudo tee -a /etc/hosts","title":"Prepare"},{"location":"start/installation-on-localhost/#start","text":"After that start ML Aide from your demo/ folder that is located in the cloned repository using: cd demo docker-compose up","title":"Start"},{"location":"start/installation-on-localhost/#verify","text":"Now you should have several containers running: MLAide web UI MLAide webserver MongoDB which is used by the webserver to store all structured metadata min.io that is used by the webserver to store all artifacts and models keycloak to provide an identity provider, authentication, and authorization","title":"Verify"},{"location":"start/installation-on-localhost/#use","text":"You can access the web UI on localhost:8880 with your browser. This demo provides three pre-defined users: adam (password = adam1, email = adam@demo.mlaide.com) bob (password = bob1, email = bob@demo.mlaide.com) eve (password = eve1, email = eve@demo.mlaide.com)","title":"Use"},{"location":"start/installation-on-localhost/#using-the-python-client","text":"To track your machine learning experiments you can use the Python client. Start the Tutorial for your first steps.","title":"Using the Python client"},{"location":"start/installation-with-helm/","text":"Installation with Helm This document describes how to install the ML Aide in your Kubernetes cluster using Helm. ML Aide requires an S3 compatible storage and MongoDB to store machine learning models and metadata. Additionally, an identity and access management (IAM) system is required. The helm chart can be configured to use external, existing installations of S3, MongoDB, and IAM or to instantiate those as part of the Helm deployment. Prerequisites Kubernetes cluster 1.19+ Helm 3.0+ Any domain where you can configure A or AAAA records Adding the Helm Repository helm repo add mlaide https://helm.mlaide.com helm repo update Configure the Chart ML Aide requires configuration of the following core functionalities: Expose ML Aide using Ingress (DNS and TLS configuration) Connection to S3 compatible storage Connection to MongoDB Connection to IAM using OpenID Connect (OIDC) 1. Expose ML Aide using Ingress NGINX Ingress TODO Google Cloud Ingress for HTTPS Load Balancing To use a Google Kubernetes Engine (GKE) Ingress use the following configuration. Reserve static IP addresses on Google Cloud Configure DNS entries Create Google managed TLS certificates Configure Ingress for webserver and UI Optional: Configure Ingress for Keycloak 1. Reserve static IP addresses on Google Cloud Google Cloud Load Balancers are assigned to a public IP address. You need to reserve the IP addresses by running the following commands. Reserve IP addresses # IP for webserver gcloud compute addresses create mlaide-webserver \\ --global \\ --ip-version IPV4 # IP for UI gcloud compute addresses create mlaide-ui \\ --global \\ --ip-version IPV4 # Optional: IP for Keycloak gcloud compute addresses create mlaide-keycloak \\ --global \\ --ip-version IPV4 2. Configure DNS entries Use your DNS management tool to configure the A-records for the three domains pointing to the reserved IP addresses. You can get the reserved IPs by using the following command: gcloud compute addresses list Assign the IPs to the following domains using your DNS configuration tool: mlaide-webserver \u2192 api.mlaide.<your-domain> mlaide-ui \u2192 mlaide.<your-domain> Optional: mlaide-keycloak \u2192 login.mlaide.<your-domain> 3. Create Google managed TLS certificates After setting the DNS entries we need to create TLS certificates to make ML Aide accessible via HTTPS. Use the following command to create the TLS certificates. Create Google-managed certificates # certificate for webserver gcloud compute ssl-certificates create mlaide-webserver-cert \\ --domains = api.mlaide.<your-domain> \\ --global # certificate for UI gcloud compute ssl-certificates create mlaide-ui-cert \\ --domains = mlaide.<your-domain> \\ --global # Optional: certificate for Keycloak gcloud compute ssl-certificates create mlaide-keycloak-cert \\ --domains = login.mlaide.<your-domain> \\ --global Creating a Google managed certificate can take up to 60 minutes according to the Google Cloud documentation . Use the following command to check the status of the requested certificates. If the PROVISIONING_STATUS shows ... the certificates have been created successfully. gcloud compute ssl-certificates list 4. Configure Ingress for webserver and UI Store the shown yaml as ingress.yaml in your working directory. Replace <your-domain> with your actual domain configured in the step above. Webserver and UI Ingress webserver : ingress : enabled : true domain : api.mlaide.<your-domain> annotations : networking.gke.io/managed-certificates : mlaide-webserver-cert kubernetes.io/ingress.global-static-ip-name : mlaide-webserver kubernetes.io/ingress.class : gce kubernetes.io/ingress.allow-http : \"false\" hosts : - host : api.mlaide.<your-domain> paths : - path : /* pathType : ImplementationSpecific ui : ingress : enabled : true domain : mlaide.<your-domain> annotations : networking.gke.io/managed-certificates : mlaide-ui-cert kubernetes.io/ingress.global-static-ip-name : mlaide-ui kubernetes.io/ingress.class : gce kubernetes.io/ingress.allow-http : \"false\" hosts : - host : mlaide.<your-domain> paths : - path : /* pathType : ImplementationSpecific 5. Optional: Configure Ingress for Keycloak If you want to use the Keycloak instance shipped with the Helm Chart, you need to add the ingress configuration for Keycloak. Add this to your ingress.yaml . Keycloak Ingress keycloak : ingress : enabled : true domain : \"login.mlaide.<your-domain>\" rules : - host : \"login.mlaide.<your-domain>\" paths : - path : /* pathType : ImplementationSpecific tls : [] annotations : networking.gke.io/managed-certificates : mlaide-keycloak-cert kubernetes.io/ingress.global-static-ip-name : mlaide-keycloak kubernetes.io/ingress.class : gce kubernetes.io/ingress.allow-http : \"false\" 2. Connection to S3 compatible storage ML Aide uses the S3 (simple storage service) API to store artifacts. You can use AWS S3 or any other S3 compatible service. The ML Aide helm charts can be installed using MinIO directly running on Kubernetes. MinIO shipped with Helm Chart Store the shown yaml as s3.yaml in your working directory and adopt it to your needs. webserver : s3 : host : \"my-release-s3\" port : \"9000\" accessKey : \"my-s3-user\" secretKey : \"my-s3-password\" # enable MinIO deployment minio : enabled : true auth : rootUser : my-s3-user rootPassword : my-s3-password AWS S3 TODO 3. Connection to MongoDB ML Aide uses the MongoDB to store projects, run, metrics, and other metadata. You can use any MongoDB instance or alternatively use MongoDB shipped as part of the helm chart. MongoDB shipped with Helm Chart Store the shown yaml as mongodb.yaml in your working directory and adopt it to your needs. webserver : mongodb : host : \"my-release-mongodb\" port : \"27017\" username : \"root\" password : \"mypassword\" database : \"mlaide\" autoIndexCreation : true authenticationDatabase : admin # enable MongoDB deployment mongodb : enabled : true auth : usernames : - my-user passwords : - my-pw databases : - mlaide rootPassword : mypassword MongoDB TODO 4. Connection to IAM Keycloak shipped with Helm Chart Store the shown yaml as iam.yaml in your working directory. Replace <your-domain> with your actual domain. # enable Keycloak deployment keycloak : enabled : true postgresql : fullnameOverride : my-release-keycloak-postgresql oidc : audience : \"https://api.mlaide.<your-domain>\" issuer : \"https://login.mlaide.<your-domain>/auth/realms/mlaide-demo\" scope : \"openid profile email offline_access\" ui : clientId : \"mlaide-k8s-demo\" webserver : jwkSetUri : \"https://login.mlaide.<your-domain>/auth/realms/mlaide-demo/protocol/openid-connect/certs\" userInfoEndpoint : \"https://login.mlaide.<your-domain>/auth/realms/mlaide-demo/protocol/openid-connect/userinfo\" nicknamePropertyName : \"preferred_username\" OpenID Connect provider TODO Installing the Chart Install the ML Aide helm chart with a release name my-release : helm install my-release mlaide/mlaide -f ingress.yaml -f s3.yaml -f mongodb.yaml -f iam.yaml Try it out Open the configured URL ( https://mlaide.<your-domain> ) for MLAide in your browser. If you have installed MLAide using the built-in Keycloak, you can log in using the pre-configured users: User: adam@example.com Password: adam1 User: bob@example.com Password: bob1 If you have used another OpenID connect provider, use any registered user within the particular provider. Upgrading the Chart If you have a running installation of MLAide using Helm, you can update configured paremeters using the follong command:: helm upgrade my-release mlaide/mlaide Uninstalling the Chart To uninstall the release my-release use the following command: helm uninstall my-release Configuration mlaide A Helm chart to install ML Aide on Kubernetes Requirements Repository Name Version https://charts.bitnami.com/bitnami minio 11.10.25 https://charts.bitnami.com/bitnami mongodb 13.1.5 https://codecentric.github.io/helm-charts keycloak 18.4.0 Values Key Type Default Description fullnameOverride string \"\" String to fully override mlaide.fullname template googleCloudPlatform object {\"enableManagedCertificate\":false,\"keycloakCertificateName\":\"mlaide-keycloak-cert\",\"uiCertificateName\":\"mlaide-ui-cert\",\"webserverCertificateName\":\"mlaide-webserver-cert\"} Google Cloud Platform specific configuration. Use this only if you are using Google Kubernetes Engine (GKE). googleCloudPlatform.enableManagedCertificate bool false Enable automatic TLS certificate management. With this option enabled Google Cloud will automatically create TLS certificates. googleCloudPlatform.keycloakCertificateName string \"mlaide-keycloak-cert\" The name of the managed certificate to be created for the built-in Keycloak. The same name must be listed in keycloak.ingress.annotations.\"networking.gke.io/managed-certificates\" . This property is only required if you are using the built-in Keycloak installation. googleCloudPlatform.uiCertificateName string \"mlaide-ui-cert\" The name of the managed certificate to be created for the UI. The same name must be listed in ui.ingress.annotations.\"networking.gke.io/managed-certificates\" . googleCloudPlatform.webserverCertificateName string \"mlaide-webserver-cert\" The name of the managed certificate to be created for the webserver. The same name must be listed in webserver.ingress.annotations.\"networking.gke.io/managed-certificates\" . imagePullSecrets list [] The name of the secret containing docker registry credentials. Secret must exist in the same namespace as the helm release. keycloak.enabled bool false Specifies whether this Helm chart should install Keycloak. minio.enabled bool false Specifies whether this Helm chart should install MinIO (S3). mongodb.enabled bool false Specifies whether this Helm chart should install MongoDB. nameOverride string \"\" String to partially override mlaide.fullname template (will maintain the release name) oidc object see below Configures OpenID Connect (OIDC) for MLAide. oidc.audience string nil The audience specified in the access token issued by the authorization server. oidc.issuer string nil The issuer URL of the authorization server. oidc.scope string nil The scopes to request during login. oidc.ui.clientId string nil The client ID of MLAide registered on the authorization server. oidc.webserver.jwkSetUri string nil The JWKS URI provided by the authorization server. oidc.webserver.nicknamePropertyName string nil The property name withing the user info JSON containing the nickname/name of the user. oidc.webserver.userInfoEndpoint string nil The User Info Endpoint URI provided by the authorization server to retrieve user details. ui.affinity object {} Pod affinity. ui.autoscaling.enabled bool false Specifies whether autoscaling should be enabled. ui.autoscaling.maxReplicas int 100 The maximum number of Pods when autoscaling is enabled. ui.autoscaling.minReplicas int 1 The minimum number of Pods when autoscaling is enabled. ui.autoscaling.targetCPUUtilizationPercentage string nil The target CPU utilization for the horizontal pod autoscaler. ui.autoscaling.targetMemoryUtilizationPercentage string nil The target memory utilization for the horizontal pod autoscaler. ui.image.pullPolicy string \"IfNotPresent\" The pull policy for the MLAide UI image. ui.image.repository string \"mlaide/web-ui\" ui.image.tag string \"latest\" The tag of the MLAide UI image. ui.ingress.annotations object {} Ingress annotations. ui.ingress.className string \"\" The name of the Ingress Class associated with the ingress. ui.ingress.enabled bool false Specifies whether a ingress should be created. ui.ingress.hosts[0].host string nil Host for the ingress rule. ui.ingress.hosts[0].paths[0].path string \"/\" Path for the Ingress rule. ui.ingress.hosts[0].paths[0].pathType string \"ImplementationSpecific\" Path Type for the Ingress rule. ui.ingress.tls list [] TLS configuration. ui.nodeSelector object {} Node labels for Pod assignment. ui.podAnnotations object {} Pod annotations for MLAide UI. ui.podSecurityContext object {} Pod security context configuration to be applied for MLAide UI. ui.replicaCount int 1 The number of replicas of the UI deployment. ui.resources object {} Pod resource requests and limits. ui.securityContext object {} Container security context configuration to be applied for MLAide UI. ui.service.port int 80 The port for the MLAide UI service. ui.service.type string \"ClusterIP\" The type of service to create for the MLAide UI. ui.serviceAccount.annotations object {} Annotations to add to the service account. ui.serviceAccount.create bool true Specifies whether a service account should be created. ui.serviceAccount.name string \"ui\" The name of the service account to use. If not set and create is true, a name is generated using the fullname template. ui.tolerations list [] Node taints to tolerate. webserver.affinity object {} Pod affinity. webserver.autoscaling.enabled bool false Specifies whether autoscaling should be enabled. webserver.autoscaling.maxReplicas int 100 The maximum number of Pods when autoscaling is enabled. webserver.autoscaling.minReplicas int 1 The minimum number of Pods when autoscaling is enabled. webserver.autoscaling.targetCPUUtilizationPercentage string nil The target CPU utilization for the horizontal pod autoscaler. webserver.autoscaling.targetMemoryUtilizationPercentage string nil The target memory utilization for the horizontal pod autoscaler. webserver.image.pullPolicy string \"IfNotPresent\" The pull policy for the MLAide webserver image. webserver.image.repository string \"mlaide/webserver\" webserver.image.tag string \"latest\" The tag of the MLAide webserver image. webserver.ingress.annotations object {} Ingress annotations. webserver.ingress.className string \"\" The name of the Ingress Class associated with the ingress. webserver.ingress.enabled bool false Specifies whether a ingress should be created. webserver.ingress.hosts[0].host string nil Host for the ingress rule. webserver.ingress.hosts[0].paths[0].path string \"/\" Path for the Ingress rule. webserver.ingress.hosts[0].paths[0].pathType string \"ImplementationSpecific\" Path Type for the Ingress rule. webserver.ingress.tls list [] TLS configuration. webserver.loggingLevel string \"INFO\" The logging level. Must be one of [TRACE, DEBUG, INFO, WARN, ERROR] webserver.mongodb.database string nil The MongoDB\u00ae database name. webserver.mongodb.host string nil The MongoDB\u00ae hostname. webserver.mongodb.password string nil The MongoDB\u00ae password. This will be stored as a kubernetes secret. webserver.mongodb.port string nil The MongoDB\u00ae port. webserver.mongodb.username string nil The MongoDB\u00ae username. This will be stored as a kubernetes secret. webserver.nodeSelector object {} Node labels for Pod assignment. webserver.podAnnotations object {} Pod annotations for MLAide webserver. webserver.podSecurityContext object {} Pod security context configuration to be applied for MLAide webserver. webserver.replicaCount int 1 The number of replicas of the webserver deployment. webserver.resources object {} Pod resource requests and limits. webserver.s3.accessKey string nil The S3 access key. This will be stored as a kubernetes secret. webserver.s3.host string nil The S3 hostname. webserver.s3.port string nil The S3 port. webserver.s3.secretKey string nil The S3 secret key. This will be stored as a kubernetes secret. webserver.securityContext object {} Container security context configuration to be applied for MLAide webserver. webserver.service.port int 80 The port for the MLAide webserver service. webserver.service.type string \"ClusterIP\" The type of service to create for the MLAide webserver. webserver.serviceAccount.annotations object {} Annotations to add to the service account. webserver.serviceAccount.create bool true Specifies whether a service account should be created. webserver.serviceAccount.name string \"webserver\" The name of the service account to use. If not set and create is true, a name is generated using the fullname template. webserver.tolerations list [] Node taints to tolerate. Autogenerated from chart metadata using helm-docs v1.11.0","title":"Installation with Helm"},{"location":"start/installation-with-helm/#installation-with-helm","text":"This document describes how to install the ML Aide in your Kubernetes cluster using Helm. ML Aide requires an S3 compatible storage and MongoDB to store machine learning models and metadata. Additionally, an identity and access management (IAM) system is required. The helm chart can be configured to use external, existing installations of S3, MongoDB, and IAM or to instantiate those as part of the Helm deployment.","title":"Installation with Helm"},{"location":"start/installation-with-helm/#prerequisites","text":"Kubernetes cluster 1.19+ Helm 3.0+ Any domain where you can configure A or AAAA records","title":"Prerequisites"},{"location":"start/installation-with-helm/#adding-the-helm-repository","text":"helm repo add mlaide https://helm.mlaide.com helm repo update","title":"Adding the Helm Repository"},{"location":"start/installation-with-helm/#configure-the-chart","text":"ML Aide requires configuration of the following core functionalities: Expose ML Aide using Ingress (DNS and TLS configuration) Connection to S3 compatible storage Connection to MongoDB Connection to IAM using OpenID Connect (OIDC)","title":"Configure the Chart"},{"location":"start/installation-with-helm/#1-expose-ml-aide-using-ingress","text":"NGINX Ingress TODO Google Cloud Ingress for HTTPS Load Balancing To use a Google Kubernetes Engine (GKE) Ingress use the following configuration. Reserve static IP addresses on Google Cloud Configure DNS entries Create Google managed TLS certificates Configure Ingress for webserver and UI Optional: Configure Ingress for Keycloak","title":"1. Expose ML Aide using Ingress"},{"location":"start/installation-with-helm/#1-reserve-static-ip-addresses-on-google-cloud","text":"Google Cloud Load Balancers are assigned to a public IP address. You need to reserve the IP addresses by running the following commands. Reserve IP addresses # IP for webserver gcloud compute addresses create mlaide-webserver \\ --global \\ --ip-version IPV4 # IP for UI gcloud compute addresses create mlaide-ui \\ --global \\ --ip-version IPV4 # Optional: IP for Keycloak gcloud compute addresses create mlaide-keycloak \\ --global \\ --ip-version IPV4","title":"1. Reserve static IP addresses on Google Cloud"},{"location":"start/installation-with-helm/#2-configure-dns-entries","text":"Use your DNS management tool to configure the A-records for the three domains pointing to the reserved IP addresses. You can get the reserved IPs by using the following command: gcloud compute addresses list Assign the IPs to the following domains using your DNS configuration tool: mlaide-webserver \u2192 api.mlaide.<your-domain> mlaide-ui \u2192 mlaide.<your-domain> Optional: mlaide-keycloak \u2192 login.mlaide.<your-domain>","title":"2. Configure DNS entries"},{"location":"start/installation-with-helm/#3-create-google-managed-tls-certificates","text":"After setting the DNS entries we need to create TLS certificates to make ML Aide accessible via HTTPS. Use the following command to create the TLS certificates. Create Google-managed certificates # certificate for webserver gcloud compute ssl-certificates create mlaide-webserver-cert \\ --domains = api.mlaide.<your-domain> \\ --global # certificate for UI gcloud compute ssl-certificates create mlaide-ui-cert \\ --domains = mlaide.<your-domain> \\ --global # Optional: certificate for Keycloak gcloud compute ssl-certificates create mlaide-keycloak-cert \\ --domains = login.mlaide.<your-domain> \\ --global Creating a Google managed certificate can take up to 60 minutes according to the Google Cloud documentation . Use the following command to check the status of the requested certificates. If the PROVISIONING_STATUS shows ... the certificates have been created successfully.","title":"3. Create Google managed TLS certificates"},{"location":"start/installation-with-helm/#gcloud-compute-ssl-certificates-list","text":"","title":"gcloud compute ssl-certificates list\n"},{"location":"start/installation-with-helm/#4-configure-ingress-for-webserver-and-ui","text":"Store the shown yaml as ingress.yaml in your working directory. Replace <your-domain> with your actual domain configured in the step above. Webserver and UI Ingress webserver : ingress : enabled : true domain : api.mlaide.<your-domain> annotations : networking.gke.io/managed-certificates : mlaide-webserver-cert kubernetes.io/ingress.global-static-ip-name : mlaide-webserver kubernetes.io/ingress.class : gce kubernetes.io/ingress.allow-http : \"false\" hosts : - host : api.mlaide.<your-domain> paths : - path : /* pathType : ImplementationSpecific ui : ingress : enabled : true domain : mlaide.<your-domain> annotations : networking.gke.io/managed-certificates : mlaide-ui-cert kubernetes.io/ingress.global-static-ip-name : mlaide-ui kubernetes.io/ingress.class : gce kubernetes.io/ingress.allow-http : \"false\" hosts : - host : mlaide.<your-domain> paths : - path : /* pathType : ImplementationSpecific","title":"4. Configure Ingress for webserver and UI"},{"location":"start/installation-with-helm/#5-optional-configure-ingress-for-keycloak","text":"If you want to use the Keycloak instance shipped with the Helm Chart, you need to add the ingress configuration for Keycloak. Add this to your ingress.yaml . Keycloak Ingress keycloak : ingress : enabled : true domain : \"login.mlaide.<your-domain>\" rules : - host : \"login.mlaide.<your-domain>\" paths : - path : /* pathType : ImplementationSpecific tls : [] annotations : networking.gke.io/managed-certificates : mlaide-keycloak-cert kubernetes.io/ingress.global-static-ip-name : mlaide-keycloak kubernetes.io/ingress.class : gce kubernetes.io/ingress.allow-http : \"false\"","title":"5. Optional: Configure Ingress for Keycloak"},{"location":"start/installation-with-helm/#2-connection-to-s3-compatible-storage","text":"ML Aide uses the S3 (simple storage service) API to store artifacts. You can use AWS S3 or any other S3 compatible service. The ML Aide helm charts can be installed using MinIO directly running on Kubernetes. MinIO shipped with Helm Chart Store the shown yaml as s3.yaml in your working directory and adopt it to your needs. webserver : s3 : host : \"my-release-s3\" port : \"9000\" accessKey : \"my-s3-user\" secretKey : \"my-s3-password\" # enable MinIO deployment minio : enabled : true auth : rootUser : my-s3-user rootPassword : my-s3-password AWS S3 TODO","title":"2. Connection to S3 compatible storage"},{"location":"start/installation-with-helm/#3-connection-to-mongodb","text":"ML Aide uses the MongoDB to store projects, run, metrics, and other metadata. You can use any MongoDB instance or alternatively use MongoDB shipped as part of the helm chart. MongoDB shipped with Helm Chart Store the shown yaml as mongodb.yaml in your working directory and adopt it to your needs. webserver : mongodb : host : \"my-release-mongodb\" port : \"27017\" username : \"root\" password : \"mypassword\" database : \"mlaide\" autoIndexCreation : true authenticationDatabase : admin # enable MongoDB deployment mongodb : enabled : true auth : usernames : - my-user passwords : - my-pw databases : - mlaide rootPassword : mypassword MongoDB TODO","title":"3. Connection to MongoDB"},{"location":"start/installation-with-helm/#4-connection-to-iam","text":"Keycloak shipped with Helm Chart Store the shown yaml as iam.yaml in your working directory. Replace <your-domain> with your actual domain. # enable Keycloak deployment keycloak : enabled : true postgresql : fullnameOverride : my-release-keycloak-postgresql oidc : audience : \"https://api.mlaide.<your-domain>\" issuer : \"https://login.mlaide.<your-domain>/auth/realms/mlaide-demo\" scope : \"openid profile email offline_access\" ui : clientId : \"mlaide-k8s-demo\" webserver : jwkSetUri : \"https://login.mlaide.<your-domain>/auth/realms/mlaide-demo/protocol/openid-connect/certs\" userInfoEndpoint : \"https://login.mlaide.<your-domain>/auth/realms/mlaide-demo/protocol/openid-connect/userinfo\" nicknamePropertyName : \"preferred_username\" OpenID Connect provider TODO","title":"4. Connection to IAM"},{"location":"start/installation-with-helm/#installing-the-chart","text":"Install the ML Aide helm chart with a release name my-release : helm install my-release mlaide/mlaide -f ingress.yaml -f s3.yaml -f mongodb.yaml -f iam.yaml","title":"Installing the Chart"},{"location":"start/installation-with-helm/#try-it-out","text":"Open the configured URL ( https://mlaide.<your-domain> ) for MLAide in your browser. If you have installed MLAide using the built-in Keycloak, you can log in using the pre-configured users: User: adam@example.com Password: adam1 User: bob@example.com Password: bob1 If you have used another OpenID connect provider, use any registered user within the particular provider.","title":"Try it out"},{"location":"start/installation-with-helm/#upgrading-the-chart","text":"If you have a running installation of MLAide using Helm, you can update configured paremeters using the follong command:: helm upgrade my-release mlaide/mlaide","title":"Upgrading the Chart"},{"location":"start/installation-with-helm/#uninstalling-the-chart","text":"To uninstall the release my-release use the following command: helm uninstall my-release","title":"Uninstalling the Chart"},{"location":"start/installation-with-helm/#configuration","text":"","title":"Configuration"},{"location":"start/installation-with-helm/#mlaide","text":"A Helm chart to install ML Aide on Kubernetes","title":"mlaide"},{"location":"start/installation-with-helm/#requirements","text":"Repository Name Version https://charts.bitnami.com/bitnami minio 11.10.25 https://charts.bitnami.com/bitnami mongodb 13.1.5 https://codecentric.github.io/helm-charts keycloak 18.4.0","title":"Requirements"},{"location":"start/installation-with-helm/#values","text":"Key Type Default Description fullnameOverride string \"\" String to fully override mlaide.fullname template googleCloudPlatform object {\"enableManagedCertificate\":false,\"keycloakCertificateName\":\"mlaide-keycloak-cert\",\"uiCertificateName\":\"mlaide-ui-cert\",\"webserverCertificateName\":\"mlaide-webserver-cert\"} Google Cloud Platform specific configuration. Use this only if you are using Google Kubernetes Engine (GKE). googleCloudPlatform.enableManagedCertificate bool false Enable automatic TLS certificate management. With this option enabled Google Cloud will automatically create TLS certificates. googleCloudPlatform.keycloakCertificateName string \"mlaide-keycloak-cert\" The name of the managed certificate to be created for the built-in Keycloak. The same name must be listed in keycloak.ingress.annotations.\"networking.gke.io/managed-certificates\" . This property is only required if you are using the built-in Keycloak installation. googleCloudPlatform.uiCertificateName string \"mlaide-ui-cert\" The name of the managed certificate to be created for the UI. The same name must be listed in ui.ingress.annotations.\"networking.gke.io/managed-certificates\" . googleCloudPlatform.webserverCertificateName string \"mlaide-webserver-cert\" The name of the managed certificate to be created for the webserver. The same name must be listed in webserver.ingress.annotations.\"networking.gke.io/managed-certificates\" . imagePullSecrets list [] The name of the secret containing docker registry credentials. Secret must exist in the same namespace as the helm release. keycloak.enabled bool false Specifies whether this Helm chart should install Keycloak. minio.enabled bool false Specifies whether this Helm chart should install MinIO (S3). mongodb.enabled bool false Specifies whether this Helm chart should install MongoDB. nameOverride string \"\" String to partially override mlaide.fullname template (will maintain the release name) oidc object see below Configures OpenID Connect (OIDC) for MLAide. oidc.audience string nil The audience specified in the access token issued by the authorization server. oidc.issuer string nil The issuer URL of the authorization server. oidc.scope string nil The scopes to request during login. oidc.ui.clientId string nil The client ID of MLAide registered on the authorization server. oidc.webserver.jwkSetUri string nil The JWKS URI provided by the authorization server. oidc.webserver.nicknamePropertyName string nil The property name withing the user info JSON containing the nickname/name of the user. oidc.webserver.userInfoEndpoint string nil The User Info Endpoint URI provided by the authorization server to retrieve user details. ui.affinity object {} Pod affinity. ui.autoscaling.enabled bool false Specifies whether autoscaling should be enabled. ui.autoscaling.maxReplicas int 100 The maximum number of Pods when autoscaling is enabled. ui.autoscaling.minReplicas int 1 The minimum number of Pods when autoscaling is enabled. ui.autoscaling.targetCPUUtilizationPercentage string nil The target CPU utilization for the horizontal pod autoscaler. ui.autoscaling.targetMemoryUtilizationPercentage string nil The target memory utilization for the horizontal pod autoscaler. ui.image.pullPolicy string \"IfNotPresent\" The pull policy for the MLAide UI image. ui.image.repository string \"mlaide/web-ui\" ui.image.tag string \"latest\" The tag of the MLAide UI image. ui.ingress.annotations object {} Ingress annotations. ui.ingress.className string \"\" The name of the Ingress Class associated with the ingress. ui.ingress.enabled bool false Specifies whether a ingress should be created. ui.ingress.hosts[0].host string nil Host for the ingress rule. ui.ingress.hosts[0].paths[0].path string \"/\" Path for the Ingress rule. ui.ingress.hosts[0].paths[0].pathType string \"ImplementationSpecific\" Path Type for the Ingress rule. ui.ingress.tls list [] TLS configuration. ui.nodeSelector object {} Node labels for Pod assignment. ui.podAnnotations object {} Pod annotations for MLAide UI. ui.podSecurityContext object {} Pod security context configuration to be applied for MLAide UI. ui.replicaCount int 1 The number of replicas of the UI deployment. ui.resources object {} Pod resource requests and limits. ui.securityContext object {} Container security context configuration to be applied for MLAide UI. ui.service.port int 80 The port for the MLAide UI service. ui.service.type string \"ClusterIP\" The type of service to create for the MLAide UI. ui.serviceAccount.annotations object {} Annotations to add to the service account. ui.serviceAccount.create bool true Specifies whether a service account should be created. ui.serviceAccount.name string \"ui\" The name of the service account to use. If not set and create is true, a name is generated using the fullname template. ui.tolerations list [] Node taints to tolerate. webserver.affinity object {} Pod affinity. webserver.autoscaling.enabled bool false Specifies whether autoscaling should be enabled. webserver.autoscaling.maxReplicas int 100 The maximum number of Pods when autoscaling is enabled. webserver.autoscaling.minReplicas int 1 The minimum number of Pods when autoscaling is enabled. webserver.autoscaling.targetCPUUtilizationPercentage string nil The target CPU utilization for the horizontal pod autoscaler. webserver.autoscaling.targetMemoryUtilizationPercentage string nil The target memory utilization for the horizontal pod autoscaler. webserver.image.pullPolicy string \"IfNotPresent\" The pull policy for the MLAide webserver image. webserver.image.repository string \"mlaide/webserver\" webserver.image.tag string \"latest\" The tag of the MLAide webserver image. webserver.ingress.annotations object {} Ingress annotations. webserver.ingress.className string \"\" The name of the Ingress Class associated with the ingress. webserver.ingress.enabled bool false Specifies whether a ingress should be created. webserver.ingress.hosts[0].host string nil Host for the ingress rule. webserver.ingress.hosts[0].paths[0].path string \"/\" Path for the Ingress rule. webserver.ingress.hosts[0].paths[0].pathType string \"ImplementationSpecific\" Path Type for the Ingress rule. webserver.ingress.tls list [] TLS configuration. webserver.loggingLevel string \"INFO\" The logging level. Must be one of [TRACE, DEBUG, INFO, WARN, ERROR] webserver.mongodb.database string nil The MongoDB\u00ae database name. webserver.mongodb.host string nil The MongoDB\u00ae hostname. webserver.mongodb.password string nil The MongoDB\u00ae password. This will be stored as a kubernetes secret. webserver.mongodb.port string nil The MongoDB\u00ae port. webserver.mongodb.username string nil The MongoDB\u00ae username. This will be stored as a kubernetes secret. webserver.nodeSelector object {} Node labels for Pod assignment. webserver.podAnnotations object {} Pod annotations for MLAide webserver. webserver.podSecurityContext object {} Pod security context configuration to be applied for MLAide webserver. webserver.replicaCount int 1 The number of replicas of the webserver deployment. webserver.resources object {} Pod resource requests and limits. webserver.s3.accessKey string nil The S3 access key. This will be stored as a kubernetes secret. webserver.s3.host string nil The S3 hostname. webserver.s3.port string nil The S3 port. webserver.s3.secretKey string nil The S3 secret key. This will be stored as a kubernetes secret. webserver.securityContext object {} Container security context configuration to be applied for MLAide webserver. webserver.service.port int 80 The port for the MLAide webserver service. webserver.service.type string \"ClusterIP\" The type of service to create for the MLAide webserver. webserver.serviceAccount.annotations object {} Annotations to add to the service account. webserver.serviceAccount.create bool true Specifies whether a service account should be created. webserver.serviceAccount.name string \"webserver\" The name of the service account to use. If not set and create is true, a name is generated using the fullname template. webserver.tolerations list [] Node taints to tolerate. Autogenerated from chart metadata using helm-docs v1.11.0","title":"Values"},{"location":"tutorial/data-preparation/","text":"Data Preparation In this chapter, we will load and prepare the USA Housing dataset . All steps that we execute will be stored in ML Aide. Download required Dataset First, download the dataset and save it in a subdirectory called data . mkdir data curl https://raw.githubusercontent.com/MLAide/docs/master/docs/tutorial/housing.csv --output ./data/housing.csv Create an API key Later, we want to send all parameters, metrics, and models of our experiments to ML Aide. Therefore, we need to set an API key in our python client. Otherwise, we won't be able to authenticate against the ML Aide server. In the upper right click on adam > Settings Go to API Keys in the left navigation Click on Add API Key Enter any description and click on Create Copy the show API key and store it somewhere safe. The API key won't be shown again. If you lose your API key you have to create a new one. Create connection to ML Aide webserver Our data preparation will be implemented in data_preparation.py . Therefore, create a new file with this name. To create a connection to the ML Aide webserver with Python clients you have to use mlaide.MLAideClient . An object of this class is the main entry point for all kinds of operations. Replace api_key with your personal API key that you created using the ML Aide web UI. from mlaide import MLAideClient , ConnectionOptions import pandas as pd options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) Create Run Before we read or process anything we should start tracking all relevant information in ML Aide. In ML Aide a run is the key concept to track parameters, metrics, artifacts and models. All runs belong to one or more experiments . run_data_preparation = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'data preparation' ) Now we can read and process this dataset. Also, we can register the dataset as an artifact in ML Aide. This gives us the ability to reproduce the following steps - even if the dataset is lost, deleted, or modified. The artifact can be used in other runs as an input. This helps to track down the lineage of a machine learning model to its root. In the end, don't forget to mark the run as completed. housing_data = pd . read_csv ( 'data/housing.csv' ) # add dataset as artifact artifact = run_data_preparation . create_artifact ( name = \"USA housing dataset\" , artifact_type = \"dataset\" , metadata = {}) run_data_preparation . add_artifact_file ( artifact , 'data/housing.csv' ) run_data_preparation . set_completed_status () Start your python script using your shell with python data_preparation.py . After the script completed check the web UI to see the created run and the artifact. Summary In this chapter we created an API key for authorization created our first run in ML Aide attached the dataset as an artifact to the run connected the python client to the ML Aide webserver Your code should look like the following snippet shows. Code data_preparation.py from mlaide import MLAideClient , ConnectionOptions import pandas as pd options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) run_data_preparation = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'data preparation' ) housing_data = pd . read_csv ( 'data/housing.csv' ) # add dataset as artifact artifact = run_data_preparation . create_artifact ( name = \"USA housing dataset\" , artifact_type = \"dataset\" , metadata = {}) run_data_preparation . add_artifact_file ( artifact , 'data/housing.csv' ) run_data_preparation . set_completed_status () data/housing.csv a lot of housing data The next step is to create a model based on this dataset.","title":"Data Preparation"},{"location":"tutorial/data-preparation/#data-preparation","text":"In this chapter, we will load and prepare the USA Housing dataset . All steps that we execute will be stored in ML Aide.","title":"Data Preparation"},{"location":"tutorial/data-preparation/#download-required-dataset","text":"First, download the dataset and save it in a subdirectory called data . mkdir data curl https://raw.githubusercontent.com/MLAide/docs/master/docs/tutorial/housing.csv --output ./data/housing.csv","title":"Download required Dataset"},{"location":"tutorial/data-preparation/#create-an-api-key","text":"Later, we want to send all parameters, metrics, and models of our experiments to ML Aide. Therefore, we need to set an API key in our python client. Otherwise, we won't be able to authenticate against the ML Aide server. In the upper right click on adam > Settings Go to API Keys in the left navigation Click on Add API Key Enter any description and click on Create Copy the show API key and store it somewhere safe. The API key won't be shown again. If you lose your API key you have to create a new one.","title":"Create an API key"},{"location":"tutorial/data-preparation/#create-connection-to-ml-aide-webserver","text":"Our data preparation will be implemented in data_preparation.py . Therefore, create a new file with this name. To create a connection to the ML Aide webserver with Python clients you have to use mlaide.MLAideClient . An object of this class is the main entry point for all kinds of operations. Replace api_key with your personal API key that you created using the ML Aide web UI. from mlaide import MLAideClient , ConnectionOptions import pandas as pd options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options )","title":"Create connection to ML Aide webserver"},{"location":"tutorial/data-preparation/#create-run","text":"Before we read or process anything we should start tracking all relevant information in ML Aide. In ML Aide a run is the key concept to track parameters, metrics, artifacts and models. All runs belong to one or more experiments . run_data_preparation = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'data preparation' ) Now we can read and process this dataset. Also, we can register the dataset as an artifact in ML Aide. This gives us the ability to reproduce the following steps - even if the dataset is lost, deleted, or modified. The artifact can be used in other runs as an input. This helps to track down the lineage of a machine learning model to its root. In the end, don't forget to mark the run as completed. housing_data = pd . read_csv ( 'data/housing.csv' ) # add dataset as artifact artifact = run_data_preparation . create_artifact ( name = \"USA housing dataset\" , artifact_type = \"dataset\" , metadata = {}) run_data_preparation . add_artifact_file ( artifact , 'data/housing.csv' ) run_data_preparation . set_completed_status () Start your python script using your shell with python data_preparation.py . After the script completed check the web UI to see the created run and the artifact.","title":"Create Run"},{"location":"tutorial/data-preparation/#summary","text":"In this chapter we created an API key for authorization created our first run in ML Aide attached the dataset as an artifact to the run connected the python client to the ML Aide webserver Your code should look like the following snippet shows. Code data_preparation.py from mlaide import MLAideClient , ConnectionOptions import pandas as pd options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) run_data_preparation = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'data preparation' ) housing_data = pd . read_csv ( 'data/housing.csv' ) # add dataset as artifact artifact = run_data_preparation . create_artifact ( name = \"USA housing dataset\" , artifact_type = \"dataset\" , metadata = {}) run_data_preparation . add_artifact_file ( artifact , 'data/housing.csv' ) run_data_preparation . set_completed_status () data/housing.csv a lot of housing data The next step is to create a model based on this dataset.","title":"Summary"},{"location":"tutorial/introduction/","text":"Introduction In this tutorial, we will use the USA Housing dataset to predict the prices of houses in Boston. To keep things simple for this tutorial we won't use Jupyter Notebooks. Instead, we write just simple Python code. We will use the dataset to train some machine learning models. All processed data and all experiments will be tracked in ML Aide. We'll also store the trained models in ML Aide and read them later to predict new values.","title":"Introduction"},{"location":"tutorial/introduction/#introduction","text":"In this tutorial, we will use the USA Housing dataset to predict the prices of houses in Boston. To keep things simple for this tutorial we won't use Jupyter Notebooks. Instead, we write just simple Python code. We will use the dataset to train some machine learning models. All processed data and all experiments will be tracked in ML Aide. We'll also store the trained models in ML Aide and read them later to predict new values.","title":"Introduction"},{"location":"tutorial/model-evaluation/","text":"Model Evaluation Compare Runs A key feature of ML Aide is to compare several runs with their parameters and metrics. Therefore open the web UI and select all runs that should be compared. In this case, we want to compare linear regression and lasso model . Select the two checkboxes and click on the Compare button at the top of the table. You will see all parameters and metrics of the two runs. All values that are not equal will be highlighted. Visualize Lineage Sometimes you want to know how a model was built or which runs (steps) were executed in a particular experiment. For this, you can use the lineage visualization of experiments. Go to the experiment view and select an experiment. You will see all runs (blue color) and all input/output artifacts (red color). The table below shows you all runs and artifacts. From the runs table, you can jump the run details or to the run comparison. Model Staging In this very basic example, we can see that both models are quite similar. We can choose any of these models and tag them as 'production ready'. This helps to keep track of models that are used in production, are still under development (or QA), or are already deprecated. ML Aide provides the following stages for models: None Staging Production Deprecated Abandoned Summary In this chapter we compared the two model training runs visualized the model lineage staged the models ML Aide provides us with tools to keep track of all machine learning experiments, the models, and their lineage. In the next chapter, we will learn how we can load models from ML Aide to predict values.","title":"Model Evaluation"},{"location":"tutorial/model-evaluation/#model-evaluation","text":"","title":"Model Evaluation"},{"location":"tutorial/model-evaluation/#compare-runs","text":"A key feature of ML Aide is to compare several runs with their parameters and metrics. Therefore open the web UI and select all runs that should be compared. In this case, we want to compare linear regression and lasso model . Select the two checkboxes and click on the Compare button at the top of the table. You will see all parameters and metrics of the two runs. All values that are not equal will be highlighted.","title":"Compare Runs"},{"location":"tutorial/model-evaluation/#visualize-lineage","text":"Sometimes you want to know how a model was built or which runs (steps) were executed in a particular experiment. For this, you can use the lineage visualization of experiments. Go to the experiment view and select an experiment. You will see all runs (blue color) and all input/output artifacts (red color). The table below shows you all runs and artifacts. From the runs table, you can jump the run details or to the run comparison.","title":"Visualize Lineage"},{"location":"tutorial/model-evaluation/#model-staging","text":"In this very basic example, we can see that both models are quite similar. We can choose any of these models and tag them as 'production ready'. This helps to keep track of models that are used in production, are still under development (or QA), or are already deprecated. ML Aide provides the following stages for models: None Staging Production Deprecated Abandoned","title":"Model Staging"},{"location":"tutorial/model-evaluation/#summary","text":"In this chapter we compared the two model training runs visualized the model lineage staged the models ML Aide provides us with tools to keep track of all machine learning experiments, the models, and their lineage. In the next chapter, we will learn how we can load models from ML Aide to predict values.","title":"Summary"},{"location":"tutorial/model-serving/","text":"Model Serving In the previous chapters, we trained two models based on the USA Housing dataset . Now we will reload the linear regression model to do some predictions. Create connection to ML Aide webserver Our code will be written in a new file named serving.py . In the beginning, we will create a connection to the ML Aide webserver. from mlaide import MLAideClient , ConnectionOptions from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression import numpy as np options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) Load Model and Pipeline To do some predictions we want to use the linear regression model. But before we predict values with the model, we have to use the sklearn pipeline to transform our input vectors. The pipeline was also stored in ML Aide. Thus, we can load both from ML Aide. # read the model lin_reg : LinearRegression = mlaide_client . load_model ( 'linear regression' ) # read the pipeline containing the standard scaler pipeline : Pipeline = mlaide_client . load_model ( 'pipeline' ) Predict Values Now we are ready to use our model. In this case, we will hardcode a house area for our prediction. In real-world scenarios, we would get the input from HTTP requests or something similar. # create some data for prediction data = np . array ([[ 80000 , 6.32 , 7.4 , 4.24 , 25000 ]]) # The values are # - Avg. Area Income # - Avg. Area House Age # - Avg. Area Number of Rooms # - Avg. Area Number of Bedrooms # - Area Population # predict the house price data = pipeline . transform ( data ) pred = lin_reg . predict ( data ) print ( pred ) # output is: [1415072.9471789] Summary In this chapter we loaded the linear regression model loaded the sklearn pipeline predicted a value using the model Your code should look like the following snippet shows. Code serving.py from mlaide import MLAideClient , ConnectionOptions from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression import numpy as np options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) # read the model lin_reg : LinearRegression = mlaide_client . load_model ( 'linear regression' ) # read the pipeline containing the standard scaler pipeline : Pipeline = mlaide_client . load_model ( 'pipeline' ) # create some data for prediction data = np . array ([[ 80000 , 6.32 , 7.4 , 4.24 , 25000 ]]) # The values are # - Avg. Area Income # - Avg. Area House Age # - Avg. Area Number of Rooms # - Avg. Area Number of Bedrooms # - Area Population # predict the house price data = pipeline . transform ( data ) pred = lin_reg . predict ( data ) print ( pred ) training.py from mlaide import MLAideClient , ConnectionOptions , ArtifactRef import pandas as pd import numpy as np from sklearn.model_selection import cross_val_score , train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression , Lasso from sklearn import metrics # create connection options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) # get housing dataset dataset_bytes = mlaide_client . get_artifact ( 'USA housing dataset' , version = None ) . load ( 'data/housing.csv' ) housing_data = pd . read_csv ( dataset_bytes ) artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) run_pipeline_setup = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'pipeline setup' , used_artifacts = [ artifact_ref ]) # train test split X = housing_data [[ 'Avg. Area Income' , 'Avg. Area House Age' , 'Avg. Area Number of Rooms' , 'Avg. Area Number of Bedrooms' , 'Area Population' ]] y = housing_data [ 'Price' ] test_size = 0.3 random_state = 42 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = test_size , random_state = random_state ) run_pipeline_setup . log_parameter ( 'test_size' , test_size ) run_pipeline_setup . log_parameter ( 'random_state' , random_state ) pipeline = Pipeline ([ ( 'std_scalar' , StandardScaler ()) ]) X_train = pipeline . fit_transform ( X_train ) X_test = pipeline . transform ( X_test ) run_pipeline_setup . log_model ( pipeline , model_name = \"pipeline\" ) run_pipeline_setup . set_completed_status () # linear regression dataset_artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) pipeline_artifact_ref = ArtifactRef ( name = \"pipeline\" , version = 1 ) run_linear_regression = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'linear regression' , used_artifacts = [ dataset_artifact_ref , pipeline_artifact_ref ]) lin_reg = LinearRegression ( normalize = True ) lin_reg . fit ( X_train , y_train ) run_linear_regression . log_model ( lin_reg , 'linear regression' ) test_pred = lin_reg . predict ( X_test ) train_pred = lin_reg . predict ( X_train ) mae = metrics . mean_absolute_error ( y_test , test_pred ) mse = metrics . mean_squared_error ( y_test , test_pred ) rmse = np . sqrt ( metrics . mean_squared_error ( y_test , test_pred )) r2 = metrics . r2_score ( y_test , test_pred ) cross_validation = cross_val_score ( LinearRegression (), X , y , cv = 10 ) . mean () run_linear_regression . log_metric ( 'mae' , mae ) run_linear_regression . log_metric ( 'mse' , mse ) run_linear_regression . log_metric ( 'rmse' , rmse ) run_linear_regression . log_metric ( 'r2' , r2 ) run_linear_regression . log_metric ( 'cross validation' , cross_validation ) run_linear_regression . set_completed_status () # lasso regression dataset_artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) pipeline_artifact_ref = ArtifactRef ( name = \"pipeline\" , version = 1 ) run_lasso = mlaide_client . start_new_run ( experiment_key = 'lasso-regression' , run_name = 'lasso regression' , used_artifacts = [ dataset_artifact_ref , pipeline_artifact_ref ]) alpha = 0.1 precompute = True positive = True selection = 'random' random_state = 42 run_lasso . log_parameter ( 'alpha' , alpha ) run_lasso . log_parameter ( 'precompute' , precompute ) run_lasso . log_parameter ( 'positive' , positive ) run_lasso . log_parameter ( 'selection' , selection ) run_lasso . log_parameter ( 'random state' , random_state ) model = Lasso ( alpha = alpha , precompute = precompute , positive = positive , selection = selection , random_state = random_state ) model . fit ( X_train , y_train ) run_lasso . log_model ( model , 'lasso' ) test_pred = model . predict ( X_test ) train_pred = model . predict ( X_train ) mae = metrics . mean_absolute_error ( y_test , test_pred ) mse = metrics . mean_squared_error ( y_test , test_pred ) rmse = np . sqrt ( metrics . mean_squared_error ( y_test , test_pred )) r2 = metrics . r2_score ( y_test , test_pred ) cross_validation = cross_val_score ( Lasso (), X , y , cv = 10 ) . mean () run_lasso . log_metric ( 'mae' , mae ) run_lasso . log_metric ( 'mse' , mse ) run_lasso . log_metric ( 'rmse' , rmse ) run_lasso . log_metric ( 'r2' , r2 ) run_lasso . log_metric ( 'cross validation' , cross_validation ) run_lasso . set_completed_status () data_preparation.py from mlaide import MLAideClient , ConnectionOptions import pandas as pd options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) run_data_preparation = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'data preparation' ) housing_data = pd . read_csv ( 'data/housing.csv' ) # add dataset as artifact artifact = run_data_preparation . create_artifact ( name = \"USA housing dataset\" , artifact_type = \"dataset\" , metadata = {}) run_data_preparation . add_artifact_file ( artifact , 'data/housing.csv' ) run_data_preparation . set_completed_status () data/housing.csv a lot of housing data","title":"Model Serving"},{"location":"tutorial/model-serving/#model-serving","text":"In the previous chapters, we trained two models based on the USA Housing dataset . Now we will reload the linear regression model to do some predictions.","title":"Model Serving"},{"location":"tutorial/model-serving/#create-connection-to-ml-aide-webserver","text":"Our code will be written in a new file named serving.py . In the beginning, we will create a connection to the ML Aide webserver. from mlaide import MLAideClient , ConnectionOptions from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression import numpy as np options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options )","title":"Create connection to ML Aide webserver"},{"location":"tutorial/model-serving/#load-model-and-pipeline","text":"To do some predictions we want to use the linear regression model. But before we predict values with the model, we have to use the sklearn pipeline to transform our input vectors. The pipeline was also stored in ML Aide. Thus, we can load both from ML Aide. # read the model lin_reg : LinearRegression = mlaide_client . load_model ( 'linear regression' ) # read the pipeline containing the standard scaler pipeline : Pipeline = mlaide_client . load_model ( 'pipeline' )","title":"Load Model and Pipeline"},{"location":"tutorial/model-serving/#predict-values","text":"Now we are ready to use our model. In this case, we will hardcode a house area for our prediction. In real-world scenarios, we would get the input from HTTP requests or something similar. # create some data for prediction data = np . array ([[ 80000 , 6.32 , 7.4 , 4.24 , 25000 ]]) # The values are # - Avg. Area Income # - Avg. Area House Age # - Avg. Area Number of Rooms # - Avg. Area Number of Bedrooms # - Area Population # predict the house price data = pipeline . transform ( data ) pred = lin_reg . predict ( data ) print ( pred ) # output is: [1415072.9471789]","title":"Predict Values"},{"location":"tutorial/model-serving/#summary","text":"In this chapter we loaded the linear regression model loaded the sklearn pipeline predicted a value using the model Your code should look like the following snippet shows. Code serving.py from mlaide import MLAideClient , ConnectionOptions from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression import numpy as np options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) # read the model lin_reg : LinearRegression = mlaide_client . load_model ( 'linear regression' ) # read the pipeline containing the standard scaler pipeline : Pipeline = mlaide_client . load_model ( 'pipeline' ) # create some data for prediction data = np . array ([[ 80000 , 6.32 , 7.4 , 4.24 , 25000 ]]) # The values are # - Avg. Area Income # - Avg. Area House Age # - Avg. Area Number of Rooms # - Avg. Area Number of Bedrooms # - Area Population # predict the house price data = pipeline . transform ( data ) pred = lin_reg . predict ( data ) print ( pred ) training.py from mlaide import MLAideClient , ConnectionOptions , ArtifactRef import pandas as pd import numpy as np from sklearn.model_selection import cross_val_score , train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression , Lasso from sklearn import metrics # create connection options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) # get housing dataset dataset_bytes = mlaide_client . get_artifact ( 'USA housing dataset' , version = None ) . load ( 'data/housing.csv' ) housing_data = pd . read_csv ( dataset_bytes ) artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) run_pipeline_setup = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'pipeline setup' , used_artifacts = [ artifact_ref ]) # train test split X = housing_data [[ 'Avg. Area Income' , 'Avg. Area House Age' , 'Avg. Area Number of Rooms' , 'Avg. Area Number of Bedrooms' , 'Area Population' ]] y = housing_data [ 'Price' ] test_size = 0.3 random_state = 42 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = test_size , random_state = random_state ) run_pipeline_setup . log_parameter ( 'test_size' , test_size ) run_pipeline_setup . log_parameter ( 'random_state' , random_state ) pipeline = Pipeline ([ ( 'std_scalar' , StandardScaler ()) ]) X_train = pipeline . fit_transform ( X_train ) X_test = pipeline . transform ( X_test ) run_pipeline_setup . log_model ( pipeline , model_name = \"pipeline\" ) run_pipeline_setup . set_completed_status () # linear regression dataset_artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) pipeline_artifact_ref = ArtifactRef ( name = \"pipeline\" , version = 1 ) run_linear_regression = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'linear regression' , used_artifacts = [ dataset_artifact_ref , pipeline_artifact_ref ]) lin_reg = LinearRegression ( normalize = True ) lin_reg . fit ( X_train , y_train ) run_linear_regression . log_model ( lin_reg , 'linear regression' ) test_pred = lin_reg . predict ( X_test ) train_pred = lin_reg . predict ( X_train ) mae = metrics . mean_absolute_error ( y_test , test_pred ) mse = metrics . mean_squared_error ( y_test , test_pred ) rmse = np . sqrt ( metrics . mean_squared_error ( y_test , test_pred )) r2 = metrics . r2_score ( y_test , test_pred ) cross_validation = cross_val_score ( LinearRegression (), X , y , cv = 10 ) . mean () run_linear_regression . log_metric ( 'mae' , mae ) run_linear_regression . log_metric ( 'mse' , mse ) run_linear_regression . log_metric ( 'rmse' , rmse ) run_linear_regression . log_metric ( 'r2' , r2 ) run_linear_regression . log_metric ( 'cross validation' , cross_validation ) run_linear_regression . set_completed_status () # lasso regression dataset_artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) pipeline_artifact_ref = ArtifactRef ( name = \"pipeline\" , version = 1 ) run_lasso = mlaide_client . start_new_run ( experiment_key = 'lasso-regression' , run_name = 'lasso regression' , used_artifacts = [ dataset_artifact_ref , pipeline_artifact_ref ]) alpha = 0.1 precompute = True positive = True selection = 'random' random_state = 42 run_lasso . log_parameter ( 'alpha' , alpha ) run_lasso . log_parameter ( 'precompute' , precompute ) run_lasso . log_parameter ( 'positive' , positive ) run_lasso . log_parameter ( 'selection' , selection ) run_lasso . log_parameter ( 'random state' , random_state ) model = Lasso ( alpha = alpha , precompute = precompute , positive = positive , selection = selection , random_state = random_state ) model . fit ( X_train , y_train ) run_lasso . log_model ( model , 'lasso' ) test_pred = model . predict ( X_test ) train_pred = model . predict ( X_train ) mae = metrics . mean_absolute_error ( y_test , test_pred ) mse = metrics . mean_squared_error ( y_test , test_pred ) rmse = np . sqrt ( metrics . mean_squared_error ( y_test , test_pred )) r2 = metrics . r2_score ( y_test , test_pred ) cross_validation = cross_val_score ( Lasso (), X , y , cv = 10 ) . mean () run_lasso . log_metric ( 'mae' , mae ) run_lasso . log_metric ( 'mse' , mse ) run_lasso . log_metric ( 'rmse' , rmse ) run_lasso . log_metric ( 'r2' , r2 ) run_lasso . log_metric ( 'cross validation' , cross_validation ) run_lasso . set_completed_status () data_preparation.py from mlaide import MLAideClient , ConnectionOptions import pandas as pd options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) run_data_preparation = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'data preparation' ) housing_data = pd . read_csv ( 'data/housing.csv' ) # add dataset as artifact artifact = run_data_preparation . create_artifact ( name = \"USA housing dataset\" , artifact_type = \"dataset\" , metadata = {}) run_data_preparation . add_artifact_file ( artifact , 'data/housing.csv' ) run_data_preparation . set_completed_status () data/housing.csv a lot of housing data","title":"Summary"},{"location":"tutorial/model-training/","text":"Model Training In the last chapter we loaded the USA Housing dataset. Now we will train two different models on this dataset. All details of the training will be tracked in ML Aide. Create connection to ML Aide webserver Our code will be written in a new file named training.py . In the beginning we will create a connection to the ML Aide webserver. from mlaide import MLAideClient , ConnectionOptions , ArtifactRef import pandas as pd import numpy as np from sklearn.model_selection import cross_val_score , train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression , Lasso from sklearn import metrics options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) Train Test Split As you can see we are implementing these steps in a different file as the data preprocessing. Therefore we somehow need to get our input data. We could read the CSV again. Or we could retrieve the CSV file from ML Aide. In this case we will read the content of the file from ML Aide. In the previous step we saved the file as an artifact with the name USA housing dataset . Ommiting the version means that we want to retrieve the latest version of the artifact. dataset_bytes = mlaide_client . get_artifact ( 'USA housing dataset' , version = None ) . load ( 'data/housing.csv' ) housing_data = pd . read_csv ( dataset_bytes ) Usually a split will be done randomly. ML Aide helps you to keep things reproducible. We start a new run to track the split. Also, we set the dataset as an input artifact. artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) run_pipeline_setup = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'pipeline setup' , used_artifacts = [ artifact_ref ]) Now we split our dataset and link all information related to the split to our run. In this case we want to track all arguments ( test_size and random_state ) of the train_test_split() function. X = housing_data [[ 'Avg. Area Income' , 'Avg. Area House Age' , 'Avg. Area Number of Rooms' , 'Avg. Area Number of Bedrooms' , 'Area Population' ]] y = housing_data [ 'Price' ] test_size = 0.3 random_state = 42 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = test_size , random_state = random_state ) run_pipeline_setup . log_parameter ( 'test_size' , test_size ) run_pipeline_setup . log_parameter ( 'random_state' , random_state ) If you have a close look at the data, you can see that all X values must be scaled, before we can use them. We use the StandardScaler of sklearn. The scaler that will be fitted here, must also be used later for predicting new values. ML Aide makes this easy by just storing the scaler (or the whole pipeline) in ML Aide as an artifact. The artifact can be loaded later in a separate process for predicting. pipeline = Pipeline ([ ( 'std_scalar' , StandardScaler ()) ]) X_train = pipeline . fit_transform ( X_train ) X_test = pipeline . transform ( X_test ) run_pipeline_setup . log_model ( pipeline , model_name = \"pipeline\" ) run_pipeline_setup . set_completed_status () Linear Regression After the train-test-split, we can fit a linear regression model. We start a new run and link dataset and the pipeline as input artifacts. dataset_artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) pipeline_artifact_ref = ArtifactRef ( name = \"pipeline\" , version = 1 ) run_linear_regression = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'linear regression' , used_artifacts = [ dataset_artifact_ref , pipeline_artifact_ref ]) Now just fit your model as usual. After that, you can log the model with log_model() in ML Aide. lin_reg = LinearRegression ( normalize = True ) lin_reg . fit ( X_train , y_train ) run_linear_regression . log_model ( lin_reg , 'linear regression' ) Finally, we calculate some model metrics. The metrics will also be tracked in ML Aide. test_pred = lin_reg . predict ( X_test ) train_pred = lin_reg . predict ( X_train ) mae = metrics . mean_absolute_error ( y_test , test_pred ) mse = metrics . mean_squared_error ( y_test , test_pred ) rmse = np . sqrt ( metrics . mean_squared_error ( y_test , test_pred )) r2 = metrics . r2_score ( y_test , test_pred ) cross_validation = cross_val_score ( LinearRegression (), X , y , cv = 10 ) . mean () run_linear_regression . log_metric ( 'mae' , mae ) run_linear_regression . log_metric ( 'mse' , mse ) run_linear_regression . log_metric ( 'rmse' , rmse ) run_linear_regression . log_metric ( 'r2' , r2 ) run_linear_regression . log_metric ( 'cross validation' , cross_validation ) run_linear_regression . set_completed_status () Lasso Regression Until now, we created three runs ( data preparation , pipeline setup , and linear regression ). All of these runs belong to the experiment linear-regression . Now we train another model type - a lasso regression model. But we want to reuse the results of the data preparation and the pipeline setup. With ML Aide this can be achieved simply by using a new experiment_key and provide the artifacts of the previous runs via used_artifacts . dataset_artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) pipeline_artifact_ref = ArtifactRef ( name = \"pipeline\" , version = 1 ) run_lasso = mlaide_client . start_new_run ( experiment_key = 'lasso-regression' , run_name = 'lasso regression' , used_artifacts = [ dataset_artifact_ref , pipeline_artifact_ref ]) We fit our model as usual. alpha = 0.1 precompute = True positive = True selection = 'random' random_state = 42 run_lasso . log_parameter ( 'alpha' , alpha ) run_lasso . log_parameter ( 'precompute' , precompute ) run_lasso . log_parameter ( 'positive' , positive ) run_lasso . log_parameter ( 'selection' , selection ) run_lasso . log_parameter ( 'random state' , random_state ) model = Lasso ( alpha = alpha , precompute = precompute , positive = positive , selection = selection , random_state = random_state ) model . fit ( X_train , y_train ) run_lasso . log_model ( model , 'lasso' ) And now we calculate some metrics for this model, too. test_pred = model . predict ( X_test ) train_pred = model . predict ( X_train ) mae = metrics . mean_absolute_error ( y_test , test_pred ) mse = metrics . mean_squared_error ( y_test , test_pred ) rmse = np . sqrt ( metrics . mean_squared_error ( y_test , test_pred )) r2 = metrics . r2_score ( y_test , test_pred ) cross_validation = cross_val_score ( Lasso (), X , y , cv = 10 ) . mean () run_lasso . log_metric ( 'mae' , mae ) run_lasso . log_metric ( 'mse' , mse ) run_lasso . log_metric ( 'rmse' , rmse ) run_lasso . log_metric ( 'r2' , r2 ) run_lasso . log_metric ( 'cross validation' , cross_validation ) run_lasso . set_completed_status () Start your python script using your shell with python training.py . After the script completed check the web UI to see the created runs and the artifact. Summary In this chapter we created a sklearn pipeline with a standard scaler trained a linear regression model trained a lasso regression model All these steps were tracked in ML Aide as separate runs. The runs included all parameters and metrics that we need for reproducibility and further investigation. The pipeline and the models are stored as artifacts in ML Aide. Your code should look like the following snippet shows. Code training.py from mlaide import MLAideClient , ConnectionOptions , ArtifactRef import pandas as pd import numpy as np from sklearn.model_selection import cross_val_score , train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression , Lasso from sklearn import metrics # create connection options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) # get housing dataset dataset_bytes = mlaide_client . get_artifact ( 'USA housing dataset' , version = None ) . load ( 'data/housing.csv' ) housing_data = pd . read_csv ( dataset_bytes ) artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) run_pipeline_setup = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'pipeline setup' , used_artifacts = [ artifact_ref ]) # train test split X = housing_data [[ 'Avg. Area Income' , 'Avg. Area House Age' , 'Avg. Area Number of Rooms' , 'Avg. Area Number of Bedrooms' , 'Area Population' ]] y = housing_data [ 'Price' ] test_size = 0.3 random_state = 42 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = test_size , random_state = random_state ) run_pipeline_setup . log_parameter ( 'test_size' , test_size ) run_pipeline_setup . log_parameter ( 'random_state' , random_state ) pipeline = Pipeline ([ ( 'std_scalar' , StandardScaler ()) ]) X_train = pipeline . fit_transform ( X_train ) X_test = pipeline . transform ( X_test ) run_pipeline_setup . log_model ( pipeline , model_name = \"pipeline\" ) run_pipeline_setup . set_completed_status () # linear regression dataset_artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) pipeline_artifact_ref = ArtifactRef ( name = \"pipeline\" , version = 1 ) run_linear_regression = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'linear regression' , used_artifacts = [ dataset_artifact_ref , pipeline_artifact_ref ]) lin_reg = LinearRegression ( normalize = True ) lin_reg . fit ( X_train , y_train ) run_linear_regression . log_model ( lin_reg , 'linear regression' ) test_pred = lin_reg . predict ( X_test ) train_pred = lin_reg . predict ( X_train ) mae = metrics . mean_absolute_error ( y_test , test_pred ) mse = metrics . mean_squared_error ( y_test , test_pred ) rmse = np . sqrt ( metrics . mean_squared_error ( y_test , test_pred )) r2 = metrics . r2_score ( y_test , test_pred ) cross_validation = cross_val_score ( LinearRegression (), X , y , cv = 10 ) . mean () run_linear_regression . log_metric ( 'mae' , mae ) run_linear_regression . log_metric ( 'mse' , mse ) run_linear_regression . log_metric ( 'rmse' , rmse ) run_linear_regression . log_metric ( 'r2' , r2 ) run_linear_regression . log_metric ( 'cross validation' , cross_validation ) run_linear_regression . set_completed_status () # lasso regression dataset_artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) pipeline_artifact_ref = ArtifactRef ( name = \"pipeline\" , version = 1 ) run_lasso = mlaide_client . start_new_run ( experiment_key = 'lasso-regression' , run_name = 'lasso regression' , used_artifacts = [ dataset_artifact_ref , pipeline_artifact_ref ]) alpha = 0.1 precompute = True positive = True selection = 'random' random_state = 42 run_lasso . log_parameter ( 'alpha' , alpha ) run_lasso . log_parameter ( 'precompute' , precompute ) run_lasso . log_parameter ( 'positive' , positive ) run_lasso . log_parameter ( 'selection' , selection ) run_lasso . log_parameter ( 'random state' , random_state ) model = Lasso ( alpha = alpha , precompute = precompute , positive = positive , selection = selection , random_state = random_state ) model . fit ( X_train , y_train ) run_lasso . log_model ( model , 'lasso' ) test_pred = model . predict ( X_test ) train_pred = model . predict ( X_train ) mae = metrics . mean_absolute_error ( y_test , test_pred ) mse = metrics . mean_squared_error ( y_test , test_pred ) rmse = np . sqrt ( metrics . mean_squared_error ( y_test , test_pred )) r2 = metrics . r2_score ( y_test , test_pred ) cross_validation = cross_val_score ( Lasso (), X , y , cv = 10 ) . mean () run_lasso . log_metric ( 'mae' , mae ) run_lasso . log_metric ( 'mse' , mse ) run_lasso . log_metric ( 'rmse' , rmse ) run_lasso . log_metric ( 'r2' , r2 ) run_lasso . log_metric ( 'cross validation' , cross_validation ) run_lasso . set_completed_status () data_preparation.py from mlaide import MLAideClient , ConnectionOptions import pandas as pd options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) run_data_preparation = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'data preparation' ) housing_data = pd . read_csv ( 'data/housing.csv' ) # add dataset as artifact artifact = run_data_preparation . create_artifact ( name = \"USA housing dataset\" , artifact_type = \"dataset\" , metadata = {}) run_data_preparation . add_artifact_file ( artifact , 'data/housing.csv' ) run_data_preparation . set_completed_status () data/housing.csv a lot of housing data In the next chapter, we will learn how to evaluate models with ML Aide.","title":"Model Training"},{"location":"tutorial/model-training/#model-training","text":"In the last chapter we loaded the USA Housing dataset. Now we will train two different models on this dataset. All details of the training will be tracked in ML Aide.","title":"Model Training"},{"location":"tutorial/model-training/#create-connection-to-ml-aide-webserver","text":"Our code will be written in a new file named training.py . In the beginning we will create a connection to the ML Aide webserver. from mlaide import MLAideClient , ConnectionOptions , ArtifactRef import pandas as pd import numpy as np from sklearn.model_selection import cross_val_score , train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression , Lasso from sklearn import metrics options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options )","title":"Create connection to ML Aide webserver"},{"location":"tutorial/model-training/#train-test-split","text":"As you can see we are implementing these steps in a different file as the data preprocessing. Therefore we somehow need to get our input data. We could read the CSV again. Or we could retrieve the CSV file from ML Aide. In this case we will read the content of the file from ML Aide. In the previous step we saved the file as an artifact with the name USA housing dataset . Ommiting the version means that we want to retrieve the latest version of the artifact. dataset_bytes = mlaide_client . get_artifact ( 'USA housing dataset' , version = None ) . load ( 'data/housing.csv' ) housing_data = pd . read_csv ( dataset_bytes ) Usually a split will be done randomly. ML Aide helps you to keep things reproducible. We start a new run to track the split. Also, we set the dataset as an input artifact. artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) run_pipeline_setup = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'pipeline setup' , used_artifacts = [ artifact_ref ]) Now we split our dataset and link all information related to the split to our run. In this case we want to track all arguments ( test_size and random_state ) of the train_test_split() function. X = housing_data [[ 'Avg. Area Income' , 'Avg. Area House Age' , 'Avg. Area Number of Rooms' , 'Avg. Area Number of Bedrooms' , 'Area Population' ]] y = housing_data [ 'Price' ] test_size = 0.3 random_state = 42 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = test_size , random_state = random_state ) run_pipeline_setup . log_parameter ( 'test_size' , test_size ) run_pipeline_setup . log_parameter ( 'random_state' , random_state ) If you have a close look at the data, you can see that all X values must be scaled, before we can use them. We use the StandardScaler of sklearn. The scaler that will be fitted here, must also be used later for predicting new values. ML Aide makes this easy by just storing the scaler (or the whole pipeline) in ML Aide as an artifact. The artifact can be loaded later in a separate process for predicting. pipeline = Pipeline ([ ( 'std_scalar' , StandardScaler ()) ]) X_train = pipeline . fit_transform ( X_train ) X_test = pipeline . transform ( X_test ) run_pipeline_setup . log_model ( pipeline , model_name = \"pipeline\" ) run_pipeline_setup . set_completed_status ()","title":"Train Test Split"},{"location":"tutorial/model-training/#linear-regression","text":"After the train-test-split, we can fit a linear regression model. We start a new run and link dataset and the pipeline as input artifacts. dataset_artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) pipeline_artifact_ref = ArtifactRef ( name = \"pipeline\" , version = 1 ) run_linear_regression = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'linear regression' , used_artifacts = [ dataset_artifact_ref , pipeline_artifact_ref ]) Now just fit your model as usual. After that, you can log the model with log_model() in ML Aide. lin_reg = LinearRegression ( normalize = True ) lin_reg . fit ( X_train , y_train ) run_linear_regression . log_model ( lin_reg , 'linear regression' ) Finally, we calculate some model metrics. The metrics will also be tracked in ML Aide. test_pred = lin_reg . predict ( X_test ) train_pred = lin_reg . predict ( X_train ) mae = metrics . mean_absolute_error ( y_test , test_pred ) mse = metrics . mean_squared_error ( y_test , test_pred ) rmse = np . sqrt ( metrics . mean_squared_error ( y_test , test_pred )) r2 = metrics . r2_score ( y_test , test_pred ) cross_validation = cross_val_score ( LinearRegression (), X , y , cv = 10 ) . mean () run_linear_regression . log_metric ( 'mae' , mae ) run_linear_regression . log_metric ( 'mse' , mse ) run_linear_regression . log_metric ( 'rmse' , rmse ) run_linear_regression . log_metric ( 'r2' , r2 ) run_linear_regression . log_metric ( 'cross validation' , cross_validation ) run_linear_regression . set_completed_status ()","title":"Linear Regression"},{"location":"tutorial/model-training/#lasso-regression","text":"Until now, we created three runs ( data preparation , pipeline setup , and linear regression ). All of these runs belong to the experiment linear-regression . Now we train another model type - a lasso regression model. But we want to reuse the results of the data preparation and the pipeline setup. With ML Aide this can be achieved simply by using a new experiment_key and provide the artifacts of the previous runs via used_artifacts . dataset_artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) pipeline_artifact_ref = ArtifactRef ( name = \"pipeline\" , version = 1 ) run_lasso = mlaide_client . start_new_run ( experiment_key = 'lasso-regression' , run_name = 'lasso regression' , used_artifacts = [ dataset_artifact_ref , pipeline_artifact_ref ]) We fit our model as usual. alpha = 0.1 precompute = True positive = True selection = 'random' random_state = 42 run_lasso . log_parameter ( 'alpha' , alpha ) run_lasso . log_parameter ( 'precompute' , precompute ) run_lasso . log_parameter ( 'positive' , positive ) run_lasso . log_parameter ( 'selection' , selection ) run_lasso . log_parameter ( 'random state' , random_state ) model = Lasso ( alpha = alpha , precompute = precompute , positive = positive , selection = selection , random_state = random_state ) model . fit ( X_train , y_train ) run_lasso . log_model ( model , 'lasso' ) And now we calculate some metrics for this model, too. test_pred = model . predict ( X_test ) train_pred = model . predict ( X_train ) mae = metrics . mean_absolute_error ( y_test , test_pred ) mse = metrics . mean_squared_error ( y_test , test_pred ) rmse = np . sqrt ( metrics . mean_squared_error ( y_test , test_pred )) r2 = metrics . r2_score ( y_test , test_pred ) cross_validation = cross_val_score ( Lasso (), X , y , cv = 10 ) . mean () run_lasso . log_metric ( 'mae' , mae ) run_lasso . log_metric ( 'mse' , mse ) run_lasso . log_metric ( 'rmse' , rmse ) run_lasso . log_metric ( 'r2' , r2 ) run_lasso . log_metric ( 'cross validation' , cross_validation ) run_lasso . set_completed_status () Start your python script using your shell with python training.py . After the script completed check the web UI to see the created runs and the artifact.","title":"Lasso Regression"},{"location":"tutorial/model-training/#summary","text":"In this chapter we created a sklearn pipeline with a standard scaler trained a linear regression model trained a lasso regression model All these steps were tracked in ML Aide as separate runs. The runs included all parameters and metrics that we need for reproducibility and further investigation. The pipeline and the models are stored as artifacts in ML Aide. Your code should look like the following snippet shows. Code training.py from mlaide import MLAideClient , ConnectionOptions , ArtifactRef import pandas as pd import numpy as np from sklearn.model_selection import cross_val_score , train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression , Lasso from sklearn import metrics # create connection options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) # get housing dataset dataset_bytes = mlaide_client . get_artifact ( 'USA housing dataset' , version = None ) . load ( 'data/housing.csv' ) housing_data = pd . read_csv ( dataset_bytes ) artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) run_pipeline_setup = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'pipeline setup' , used_artifacts = [ artifact_ref ]) # train test split X = housing_data [[ 'Avg. Area Income' , 'Avg. Area House Age' , 'Avg. Area Number of Rooms' , 'Avg. Area Number of Bedrooms' , 'Area Population' ]] y = housing_data [ 'Price' ] test_size = 0.3 random_state = 42 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = test_size , random_state = random_state ) run_pipeline_setup . log_parameter ( 'test_size' , test_size ) run_pipeline_setup . log_parameter ( 'random_state' , random_state ) pipeline = Pipeline ([ ( 'std_scalar' , StandardScaler ()) ]) X_train = pipeline . fit_transform ( X_train ) X_test = pipeline . transform ( X_test ) run_pipeline_setup . log_model ( pipeline , model_name = \"pipeline\" ) run_pipeline_setup . set_completed_status () # linear regression dataset_artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) pipeline_artifact_ref = ArtifactRef ( name = \"pipeline\" , version = 1 ) run_linear_regression = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'linear regression' , used_artifacts = [ dataset_artifact_ref , pipeline_artifact_ref ]) lin_reg = LinearRegression ( normalize = True ) lin_reg . fit ( X_train , y_train ) run_linear_regression . log_model ( lin_reg , 'linear regression' ) test_pred = lin_reg . predict ( X_test ) train_pred = lin_reg . predict ( X_train ) mae = metrics . mean_absolute_error ( y_test , test_pred ) mse = metrics . mean_squared_error ( y_test , test_pred ) rmse = np . sqrt ( metrics . mean_squared_error ( y_test , test_pred )) r2 = metrics . r2_score ( y_test , test_pred ) cross_validation = cross_val_score ( LinearRegression (), X , y , cv = 10 ) . mean () run_linear_regression . log_metric ( 'mae' , mae ) run_linear_regression . log_metric ( 'mse' , mse ) run_linear_regression . log_metric ( 'rmse' , rmse ) run_linear_regression . log_metric ( 'r2' , r2 ) run_linear_regression . log_metric ( 'cross validation' , cross_validation ) run_linear_regression . set_completed_status () # lasso regression dataset_artifact_ref = ArtifactRef ( name = \"USA housing dataset\" , version = 1 ) pipeline_artifact_ref = ArtifactRef ( name = \"pipeline\" , version = 1 ) run_lasso = mlaide_client . start_new_run ( experiment_key = 'lasso-regression' , run_name = 'lasso regression' , used_artifacts = [ dataset_artifact_ref , pipeline_artifact_ref ]) alpha = 0.1 precompute = True positive = True selection = 'random' random_state = 42 run_lasso . log_parameter ( 'alpha' , alpha ) run_lasso . log_parameter ( 'precompute' , precompute ) run_lasso . log_parameter ( 'positive' , positive ) run_lasso . log_parameter ( 'selection' , selection ) run_lasso . log_parameter ( 'random state' , random_state ) model = Lasso ( alpha = alpha , precompute = precompute , positive = positive , selection = selection , random_state = random_state ) model . fit ( X_train , y_train ) run_lasso . log_model ( model , 'lasso' ) test_pred = model . predict ( X_test ) train_pred = model . predict ( X_train ) mae = metrics . mean_absolute_error ( y_test , test_pred ) mse = metrics . mean_squared_error ( y_test , test_pred ) rmse = np . sqrt ( metrics . mean_squared_error ( y_test , test_pred )) r2 = metrics . r2_score ( y_test , test_pred ) cross_validation = cross_val_score ( Lasso (), X , y , cv = 10 ) . mean () run_lasso . log_metric ( 'mae' , mae ) run_lasso . log_metric ( 'mse' , mse ) run_lasso . log_metric ( 'rmse' , rmse ) run_lasso . log_metric ( 'r2' , r2 ) run_lasso . log_metric ( 'cross validation' , cross_validation ) run_lasso . set_completed_status () data_preparation.py from mlaide import MLAideClient , ConnectionOptions import pandas as pd options = ConnectionOptions ( server_url = 'http://localhost:8881/api/v1' , # the ML Aide demo server runs on port 8881 per default api_key = '<your api key>' ) mlaide_client = MLAideClient ( project_key = 'usa-housing' , options = options ) run_data_preparation = mlaide_client . start_new_run ( experiment_key = 'linear-regression' , run_name = 'data preparation' ) housing_data = pd . read_csv ( 'data/housing.csv' ) # add dataset as artifact artifact = run_data_preparation . create_artifact ( name = \"USA housing dataset\" , artifact_type = \"dataset\" , metadata = {}) run_data_preparation . add_artifact_file ( artifact , 'data/housing.csv' ) run_data_preparation . set_completed_status () data/housing.csv a lot of housing data In the next chapter, we will learn how to evaluate models with ML Aide.","title":"Summary"},{"location":"tutorial/setup/","text":"Setup Before we start the actual work, we need to set up our project. Run ML Aide For this tutorial, ML Aide server and web-UI are required. The Getting Started tutorial helps to get ML Aide running. Create a workspace directory In this tutorial, we will use ~/mlaide-tutorial/ as our working directory. Prepare environment For this tutorial, we recommend a Python environment manager like virtualenv in combination with pyenv . Of course, you don't have to use virtualenv or you can use any other environment manager. But in all cases, you should install the dependencies from step 6. Open a terminal and navigate to the project directory cd ~/mlaide-tutorial Install Python 3.9 (if not already present) via pyenv pyenv install Install virtualenv (if not already present) pip install virtualenv Create virtual environment virtualenv .venv Activate virtual environment source .venv/bin/activate Install all dependencies pip install scikit-learn pandas numpy mlaide Create ML Aide project Open the ML Aide web UI on localhost:8880 Login as adam (username = adam ; password = adam1 ) Click on Add Project to create a new project - enter USA Housing as project name Summary In this chapter we have set up our working environment created our tutorial project Next, we will load and prepare the dataset. Here, we will use the snippet. Now you should have everything up and running to start coding.","title":"Setup"},{"location":"tutorial/setup/#setup","text":"Before we start the actual work, we need to set up our project.","title":"Setup"},{"location":"tutorial/setup/#run-ml-aide","text":"For this tutorial, ML Aide server and web-UI are required. The Getting Started tutorial helps to get ML Aide running.","title":"Run ML Aide"},{"location":"tutorial/setup/#create-a-workspace-directory","text":"In this tutorial, we will use ~/mlaide-tutorial/ as our working directory.","title":"Create a workspace directory"},{"location":"tutorial/setup/#prepare-environment","text":"For this tutorial, we recommend a Python environment manager like virtualenv in combination with pyenv . Of course, you don't have to use virtualenv or you can use any other environment manager. But in all cases, you should install the dependencies from step 6. Open a terminal and navigate to the project directory cd ~/mlaide-tutorial Install Python 3.9 (if not already present) via pyenv pyenv install Install virtualenv (if not already present) pip install virtualenv Create virtual environment virtualenv .venv Activate virtual environment source .venv/bin/activate Install all dependencies pip install scikit-learn pandas numpy mlaide","title":"Prepare environment"},{"location":"tutorial/setup/#create-ml-aide-project","text":"Open the ML Aide web UI on localhost:8880 Login as adam (username = adam ; password = adam1 ) Click on Add Project to create a new project - enter USA Housing as project name","title":"Create ML Aide project"},{"location":"tutorial/setup/#summary","text":"In this chapter we have set up our working environment created our tutorial project Next, we will load and prepare the dataset. Here, we will use the snippet. Now you should have everything up and running to start coding.","title":"Summary"}]}